[
["index.html", "Rethomics, a framework for high-throughput behaviour analysis in R Introduction", " Rethomics, a framework for high-throughput behaviour analysis in R Quentin Geissmann 2019-07-17 Introduction Only if we share a common data structure can we use a common set of tools rethomics is a free academic software, don’t hesistate to cite rethomics if you use it in your research. In the last few years, there has been growing interests in ethomics – that is, the analysis of large behavioural data sets. Many software and hardware solutions have been proposed to record different behavioural variables on several model organisms. Although subsequent analysis and visualisation share many similarities, each method tends to provide its own output format and, in practice, its own restricted analysis software. This results in a lot of replicated work but also limits extension and collaboration. The rethomics framework unify behaviour analysis over multiple platforms Rethomics attempts to unify analysis of behaviour by providing several packages: behavr tables – a flexible and universal structure to handle very large behavioural data sets damr, scopr, … – to load data from DAMS, ethoscopes and others into behavr tables ggetho – based on ggplot2, to produce high-quality representations of behavioural data sleepr, zeitgebr, … – to analyse behavioural data (sleep analysis, circadian rhythm, …) This document is a tutorial intended for experimenters as well as data analysts. It provides a suite of both conceptual explanations and very concrete examples. "],
["intro.html", "Package List and First Steps Getting R The rethomics packages Installation", " Package List and First Steps Getting R If you have never used or heard of R before, I suggest you start by reading about data science in R and installing RStudio. Only once you have done that can we continue. The rethomics packages Rethomics works as a collection of interconnected packages. Here is a list of all our packages so far (as well as their individual PDF documentation, description and status). Package Doc Description Travis Coverage CRAN behavr Canonical Data Structure for Behavioural Data ggetho Visualisation of High-Throughput Behavioural (i.e. Ethomics) Data damr Interface to Drosophila Activity Monitor System Result Files scopr Read Ethoscope Data sleepr Analyse Activity and Sleep Behaviour zeitgebr Analysis of Circadian Behaviours Installation CRAN You may have noticed, in the table above, that some packages are on the CRAN (the official R package repository). Therefore, we can use R’s default tools to install them. For instance, behavr, can simply be installed with: install.packages(&quot;behavr&quot;) devtools For packages that are not on CRAN, or if you want the very latest snapshot of a package, you can use devtools. First, install and load it: install.packages(&quot;devtools&quot;) library(devtools) You should see a warning message saying something like &quot;GitHub repo contains submodules, [...]&quot;, which is fine. Beyond that, ensure you have no error messages. devtool is a package that simplifies installation of unofficial packages (for instance hosted on github) like rethomics ones. To install behavr, you would run: install_github(&quot;rethomics/behavr&quot;) In the same way, you could install other rethomics packages by changing &quot;behavr&quot; for another package name. For instance, you could install ggetho with install_github(&quot;rethomics/ggetho&quot;). "],
["workflow.html", "The rethomics workflow", " The rethomics workflow From hypothesis to results The rethomics workflow In rethomics, we envisage behavioural experiments as a workflow: Design – you plan your experiment (I can’t really help you with that, but I trust you!). Record/track – you use your acquisition platform to record behavioural variables over time. They define the format of the results. Write individual information – you make a spreadsheet (CSV file) that details the experimental conditions for each individual. We call this a metadata file. It is a crucial concept in rethomics, so we will dedicate it the next section. You can often write your metadata as you plan your experiment, but sometimes, you want to enrich it with variables that you can only record after your experiment (e.g. lifespan). Link and Load data – first, we enrich your metadata by “linking” it to the result. This allows you to load all the matching data into a singlebehavr table (see section on behavr tables). Tranform &amp; analyse &amp; visualise – you take advantage of rethomics and R analysis and visualisation tools. "],
["metadata.html", "Working with metadata files What are metadata? Make them exhaustive Put replicates together Linking metadata Take home message", " Working with metadata files Using and understanding metadata files makes your analyses more transparent and tracktable Schematic of a metadata file What are metadata? When performing many experiments, with multiple condidions and replicates, it becomes challenging to keep track of each individual and to link it to its actual data. In rethomics, regardless of the tool used to generate data, loading results always involves a metadata file. It is, in fact, a simple CSV file (basically a spreadsheet) in which each row defines one unique individual. As shown in the figure above, metadata is classified in two types of columns: Mandatory techincal columns – for instance date, machine_name and others (depending on the acquisition platform). They will be used to match an animal to its data. As their name suggests, they have to be filled. They contain the minimum information that the computer needs to sort your data. Optional experimental columns – in this example, condition and sex. You can use as all the columns you want to characterise your experiments. Make them exhaustive It is a good habit to record as much information as possible in the metadata file – even if it seems redundant. For instance, if we put animals in different incubators, we can simply add an incubator column. This way, we keep all our experimental notes, as much as possible, inside one file. Not only will this help us to “debug” if anything goes wrong in one incubator, but we will also be able to account for incubator as a covariate later on. From a computational perspective, having these extra columns is virtually free as they will not impact memory or processing time down the line. Put replicates together A common mistake for users is to perform several replicates of the same experiment and to make a new metadata file each time. Instead, I strongly recommand you to put all replicates in the same file. If it helps, you can add a replicate column so you can keep track of which replicate each animal comes from. The whole point of high-througput analysis is that you can load all the data from all replicates and compare it (and maybe merge it). The bottom line is that, if you start form a single metadata file, your work will be more trackable, and you can always decide to analyse only one replicate at a time. Think about this metadata file as something needed in a line of research or project more than a file needed for every experiment. Linking metadata Linking adds technical columns Once your metadata is ready, it can be used to as a query to import the matching data. Regardless of which acquisition tool you used, the first step when importing data in R will be “metadata linking”. This step will automatically complete the metadata file in a way that can be used for R to retrieve the right amount of data from experiment files. We call it linking, since it links the manually introduced metadata with the correct data file, something that is tedious to do manually. In short, linking means at least: Adding an id column to the metadata. This will be a unique identifier for each indididual (it generally contains datetime, machine name an region id). This will diferenciate animals with the same conditions in the metadata. Adding a column that tracks “how to find the data for each individual”( i.e. local or remote path the the data file) Take home message In conclusion, metadatafiles are a canonical way to both define experimental condition and load behavioural data. They are both computer and human friendly. In other words, if you pass a query on to a colaborator, she/he will be able to tell very quickly what individuals underwent which treatment, where and when. "],
["behavr.html", "behavr tables Variables and metavariables Operating on behavr tables Playing with toy data Generalities", " behavr tables A single data structure to store data and metadata a behavr table Variables and metavariables As we have seen in the previous section, metadata are crucial for proper statistical analysis of the experimental data. In the context of ethomics, the data are long time series of recorded variables such as position, orientation and number of beam crosses, for each individual. Variables are different form metavariables in so far as the latter are made of only one value per animal. It is easier (and less error prone) to always keep the data and metadata together. In rethomics, in order to handle large amounts of data (together with metadata), we have designed the behavr package. behavr tables are based on the very powerful package data.table, but enhanced with metadata. A behavr table is, indeed, formed internally by two tables: the metadata table and the data table, both are linked by the id column (see figure above). For most purposes, you can use a behavr table just like a data.table. Therefore, do take a look at the introduction to data.table for further details! When we load any behavioural data in rethomics, we get a behavr table as a result. In this section, we will discuss the usual operations that you can perform on behavr tables. Operating on behavr tables Now that we have all our data at the same place, we want to be able to manipulate it. In the next part of this tutorial, we will create some toy data and learn how to manipulate it. This is where basic knowledge of data.table comes in handy. The following table is an overview of operations in behavr tables. DT represents an behavr table. Section Operation Expression Example Generalities Summarise behavr table summary(DT) How many individuals, variables, metavariables, etc? – summary(dt) Pure data Create/alter a variable DT[, new_column := some_value] When are animals ‘very active’? – dt[, very_active := activity &gt;= 2] Remove a variable DT[, column_to_delete := NULL] Lets remove a variable we don’t need? – dt[, very_active := NULL] Select data rows DT[criteria] Exclude data before the first hour – small_dt &lt;- dt[t &gt; hours(1)] Pure metadata Access metadata table DT[meta = TRUE] Show metadata as table – dt[meta = TRUE] Create/alter metavariable DT[, new_meta := some_value, meta=TRUE] Define a new factor that is a comibiation of ‘sex x condition’ – dt[, treatment := paste(sex, condition, sep='|'), meta=T] Meta &amp; data Use metavariable as variable xmv(metavariable) Add 10s to all time, only for animals in condition 'A' – dt[, t := ifelse(xmv(condition) == 'A', t + 10, t)] Remove individuals according to metavariable DT[criteria] Remove all males (from data, and metadata) – dt_females &lt;- dt[xmv(sex) == 'females'] Summarise Compute individual statistics DT[, .( statistics = some_math()), by='id'] Compute the average activity, per animal – stat_dt &lt;- dt[, .(mean_acti = mean(active)), by='id'] Rejoin metadata to data rejoin(DT) Merge metadata and summary statistics – stat_dt &lt;- rejoin(stat_dt) Advanced Stitch experiments stitch_on(DT, metavariable) TODO – TODO Playing with toy data The behavr package has a set of functions to make toy data. This provides us with a playgound to test functions and plots without having to get any real data. In order to understand behavr object, lets create a toy one. First, we make some dummy metadata (always needed to create a behavr table): library(behavr) ## Loading required package: data.table metadata &lt;- data.table( id = paste(&quot;toy_experiment&quot;, 1:10, sep = &quot;|&quot;), sex = rep(c(&quot;male&quot;, &quot;female&quot;), each = 5), condition = c(&quot;A&quot;, &quot;B&quot;) ) metadata ## id sex condition ## 1: toy_experiment|1 male A ## 2: toy_experiment|2 male B ## 3: toy_experiment|3 male A ## 4: toy_experiment|4 male B ## 5: toy_experiment|5 male A ## 6: toy_experiment|6 female B ## 7: toy_experiment|7 female A ## 8: toy_experiment|8 female B ## 9: toy_experiment|9 female A ## 10: toy_experiment|10 female B This metadata describes an hypothetical experiment with ten animals (1:10, five males and five females). They are exposed to two conditions (&quot;A&quot; and &quot;B&quot;). Then, we use toy_dam_data() to simulate (instead of linking/loading) one day of DAMS-like data for these ten animals (and two conditions): dt &lt;- toy_dam_data(metadata, duration = days(1)) dt ## ## ==== METADATA ==== ## ## id sex condition ## &lt;char&gt; &lt;char&gt; &lt;char&gt; ## 1: toy_experiment|1 male A ## 2: toy_experiment|10 female B ## 3: toy_experiment|2 male B ## 4: toy_experiment|3 male A ## 5: toy_experiment|4 male B ## 6: toy_experiment|5 male A ## 7: toy_experiment|6 female B ## 8: toy_experiment|7 female A ## 9: toy_experiment|8 female B ## 10: toy_experiment|9 female A ## ## ====== DATA ====== ## ## id t activity ## &lt;char&gt; &lt;num&gt; &lt;int&gt; ## 1: toy_experiment|1 0 0 ## 2: toy_experiment|1 60 2 ## 3: toy_experiment|1 120 0 ## 4: toy_experiment|1 180 1 ## 5: toy_experiment|1 240 0 ## --- ## 14406: toy_experiment|9 86160 0 ## 14407: toy_experiment|9 86220 0 ## 14408: toy_experiment|9 86280 2 ## 14409: toy_experiment|9 86340 1 ## 14410: toy_experiment|9 86400 0 As you can see, when we print dt, our behavr table, we have two sections: METADATA and DATA. The former is actually just the metadata we created whilst the latter stores the data (i.e. the variables) for all animals. The special column id is also known as a key, and is shared between both data and metadata. It internally allows us to map them to one another. In other words, it is a unique id for each individual. In this specific example, the variables t and activity are the time and the number of beam crosses, respectively. Generalities A quick way to retreive general information about a behavr table is to use summary: summary(dt) ## behavr table with: ## 10 individuals ## 2 metavariables ## 2 variables ## 1.441e+04 measurements ## 1 key (id) This tells us immediately how many variables, metavariables and data points, we have. One can also print a detailed summary (i.e. one per animal): summary(dt, detailed = TRUE) ## ## Summary of each individual (one per row): ## id sex condition data_points time_range ## 1: toy_experiment|1 male A 1441 [0 -&gt; 86400 (86400)] ## 2: toy_experiment|10 female B 1441 [0 -&gt; 86400 (86400)] ## 3: toy_experiment|2 male B 1441 [0 -&gt; 86400 (86400)] ## 4: toy_experiment|3 male A 1441 [0 -&gt; 86400 (86400)] ## 5: toy_experiment|4 male B 1441 [0 -&gt; 86400 (86400)] ## 6: toy_experiment|5 male A 1441 [0 -&gt; 86400 (86400)] ## 7: toy_experiment|6 female B 1441 [0 -&gt; 86400 (86400)] ## 8: toy_experiment|7 female A 1441 [0 -&gt; 86400 (86400)] ## 9: toy_experiment|8 female B 1441 [0 -&gt; 86400 (86400)] ## 10: toy_experiment|9 female A 1441 [0 -&gt; 86400 (86400)] Data Playing with variables is just like in data.table. Read the official data.table tutorial for more functionalities. For instance, we can add a new variable, very_active, that is TRUE if and only if there was at least two beam crosses in a minute, for a given individual: dt[, very_active := activity &gt;= 2] If we decide we don’t need this variable anymore, we can remove it: dt[, very_active := NULL] Sometimes, we would like to filter the data. That is, we select rows according to one or several criteria. Often we would like to exclude the very start of the experiment. For example, we can keep data after one hour: dt &lt;- dt[ t &gt; hours(1)] Note that that using dt &lt;- mean we make a new table that overwrite the old one (since it has the same name). Metadata In order to access the metadata, we can add meta = TRUE inside the []: dt[meta = TRUE] ## id sex condition ## 1: toy_experiment|1 male A ## 2: toy_experiment|10 female B ## 3: toy_experiment|2 male B ## 4: toy_experiment|3 male A ## 5: toy_experiment|4 male B ## 6: toy_experiment|5 male A ## 7: toy_experiment|6 female B ## 8: toy_experiment|7 female A ## 9: toy_experiment|8 female B ## 10: toy_experiment|9 female A This way, we can also create new metavariables. For instance, say you want to collapse sex and condition which both have two levels into one treatment, with four levels: dt[, treatment := paste(sex, condition, sep=&#39;|&#39;), meta=T] # just to show the result: dt[meta = TRUE] ## id sex condition treatment ## 1: toy_experiment|1 male A male|A ## 2: toy_experiment|10 female B female|B ## 3: toy_experiment|2 male B male|B ## 4: toy_experiment|3 male A male|A ## 5: toy_experiment|4 male B male|B ## 6: toy_experiment|5 male A male|A ## 7: toy_experiment|6 female B female|B ## 8: toy_experiment|7 female A female|A ## 9: toy_experiment|8 female B female|B ## 10: toy_experiment|9 female A female|A paste() is a function that links strings of characters with an arbitrary separator (&quot;|&quot; here). New metavariables can also be added from a summary (see Summarise data). ### Data &amp; Metadata {-} The strength of behavr tables is their ability to seamlessly use metavariables as though they were variables. For the sake of the example, let’s say you would like to alter the variable t (time) so that we add ten seconds, only to individuals that have condition 'A'. dt[, t := ifelse(xmv(condition) == &#39;A&#39;, t + 10, t)] The key here is the use of xmv (eXpand MetaVariable), which maps condition back in the data. We can also use this mechanism to remove individuals according to the value of a metavariable. For instance, lets get rid of the males! dt &lt;- dt[xmv(sex) == &#39;female&#39;] summary(dt) ## behavr table with: ## 5 individuals ## 3 metavariables ## 2 variables ## 6.9e+03 measurements ## 1 key (id) When individuals are removed, metadata is automatically updated. In effect, we removed males from both data and metadata. This operation cannot be undone, as we overwrite dt with a new value. An alternative would be to save the result in a new table (e.g. dt_females &lt;- dt[xmv(sex) == 'female']) This would use some additional memory, but it is safer. Summarise data Thanks to data.table by operations, it is simple and efficient to compute statistics per individual. For instance, we may want to compute the average activity for each animal: stat_dt &lt;- dt[, .(mean_acti = mean(activity)), by=&#39;id&#39;] stat_dt ## ## ==== METADATA ==== ## ## id sex condition treatment ## &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; ## 1: toy_experiment|10 female B female|B ## 2: toy_experiment|6 female B female|B ## 3: toy_experiment|7 female A female|A ## 4: toy_experiment|8 female B female|B ## 5: toy_experiment|9 female A female|A ## ## ====== DATA ====== ## ## id mean_acti ## &lt;char&gt; &lt;num&gt; ## 1: toy_experiment|10 0.4420290 ## 2: toy_experiment|6 0.1615942 ## 3: toy_experiment|7 0.4434783 ## 4: toy_experiment|8 0.1731884 ## 5: toy_experiment|9 0.2550725 You can actually compute many variables in one go this way: stat_dt &lt;- dt[, .(mean_acti = mean(activity), max_acti = max(activity) ), by=&#39;id&#39;] stat_dt ## ## ==== METADATA ==== ## ## id sex condition treatment ## &lt;char&gt; &lt;char&gt; &lt;char&gt; &lt;char&gt; ## 1: toy_experiment|10 female B female|B ## 2: toy_experiment|6 female B female|B ## 3: toy_experiment|7 female A female|A ## 4: toy_experiment|8 female B female|B ## 5: toy_experiment|9 female A female|A ## ## ====== DATA ====== ## ## id mean_acti max_acti ## &lt;char&gt; &lt;num&gt; &lt;int&gt; ## 1: toy_experiment|10 0.4420290 3 ## 2: toy_experiment|6 0.1615942 2 ## 3: toy_experiment|7 0.4434783 3 ## 4: toy_experiment|8 0.1731884 2 ## 5: toy_experiment|9 0.2550725 2 Then, if needed, this summary can be added back to the metadata: # create new metadata table by joining current meta and the summary table new_meta &lt;- dt[stat_dt, meta=T] # set new metadata setmeta(dt, new_meta) head(dt[meta=T]) ## id sex condition treatment mean_acti max_acti ## 1: toy_experiment|10 female B female|B 0.4420290 3 ## 2: toy_experiment|6 female B female|B 0.1615942 2 ## 3: toy_experiment|7 female A female|A 0.4434783 3 ## 4: toy_experiment|8 female B female|B 0.1731884 2 ## 5: toy_experiment|9 female A female|A 0.2550725 2 This way we can store per-individual aggregates and visualise or analyse them with respect to the pre-existing metadata. Now, in order to perform statistics, we would like to merge our summaries to the metadata. This way we end up with only one data.table That is, we want to rejoin them to one another (i.e. we enrich our summaries with the metadata): final_dt &lt;- rejoin(stat_dt) final_dt ## id sex condition treatment mean_acti max_acti ## 1: toy_experiment|10 female B female|B 0.4420290 3 ## 2: toy_experiment|6 female B female|B 0.1615942 2 ## 3: toy_experiment|7 female A female|A 0.4434783 3 ## 4: toy_experiment|8 female B female|B 0.1731884 2 ## 5: toy_experiment|9 female A female|A 0.2550725 2 This table is exactly what you need for statistics and visualisation in R! "],
["damr.html", "DAM2 data, in practice Aims Prerequisites Background Getting the data From experiment design to metadata Linking Loading Note on datetime Quality control Apply functions when loading Next steps", " DAM2 data, in practice A matter of metadata A DAM experiment. Two replicates, 3 days of recording each; 10 days apart; three genotypes; two sexes, males and females Aims In this practical chapter, we will use a real experiment to learn how to: Translate your experiment design into a metadata file Use this metadata file to load some data Set the zeitgeber reference (ZT0) Assess graphically the quality of the data Use good practices to exclude individuals from our experiments Prerequisites You are familiar with the TriKineticks DAM system Ensure you have read about the rethomics workflow and metadata Ensure you have installed behavr, damr and ggetho packages: library(devtools) install_github(&quot;rethomics/behavr&quot;) install_github(&quot;rethomics/damr&quot;) install_github(&quot;rethomics/ggetho&quot;) Background Drosophila Activity Monitors (DAMs) are a wildely used tool to monitor activity of fruit flies over several days. I am assuming that, if you are reading this tutorial, you are already familiar with the system, but I will make a couple of points clear before we start something more hands-on: This tutorial is about single beam DAM2 but will adapt very well to multibeam DAM5. We work with the raw data (the data from each monitor is in one single file, and all the monitor files are in the same folder) Getting the data For this tutorial, you need to download some DAM2 data that we have made available. This is just a zip archive containing four files. Download and extract the files from the zip into a folder of your choice. Store the path in a variable. For instance, adapt something like: DATA_DIR &lt;- &quot;C:/Where/My/Zip/Has/been/extracted Check that all four files live there: list.files(DATA_DIR, pattern= &quot;*.txt|*.csv&quot;) ## [1] &quot;metadata.csv&quot; &quot;Monitor11.txt&quot; &quot;Monitor14.txt&quot; &quot;Monitor64.txt&quot; For this exercise, we will work with the data and metadata in the same place. However, in practice, I recommend to: Have all raw data from your acquisition platform in the same place (possibly shared with others or a network drive) Have one folder per “experiment”. That is a folder that contains one metadata file, your R scripts, your figures regarding a set of consistent experiment. For now, we can just set our working directory to DATA_DIR: setwd(DATA_DIR) From experiment design to metadata Our toy experiment A DAM experiment. Two replicates, 3 days of recording each; 10 days apart; three genotypes; two sexes, males and females In this example data, we were interested in comparing the behaviour of populations of fruit flies, according to their sex and genotype. We designed the experiment as shown is the figure above. In summary, we have: three genotypes (A, B and C) two sexes (male and female) two replicates (2017-07-01 -&gt; 2017-07-04 and 2017-07-11 -&gt; 2017-07-14) Altogether, 192 individuals Metadata It is crucial that you have read metadata chapter to understand this part. Our goal is to encode our whole experiment in a single file in which: each row is an individual each column is a metavariable Luckily for you, I have already put this file together for you as metadata.csv! Lets have a look at it (you can use R, excel or whatever you want). If you are using R, type this commands: library(damr) metadata &lt;- fread(&quot;metadata.csv&quot;) metadata ## file start_datetime stop_datetime region_id sex ## 1: Monitor11.txt 2017-07-01 08:00:00 2017-07-04 1 M ## 2: Monitor11.txt 2017-07-01 08:00:00 2017-07-04 2 M ## 3: Monitor11.txt 2017-07-01 08:00:00 2017-07-04 3 M ## 4: Monitor11.txt 2017-07-01 08:00:00 2017-07-04 4 M ## 5: Monitor11.txt 2017-07-01 08:00:00 2017-07-04 5 M ## --- ## 188: Monitor64.txt 2017-07-11 08:00:00 2017-07-14 28 F ## 189: Monitor64.txt 2017-07-11 08:00:00 2017-07-14 29 F ## 190: Monitor64.txt 2017-07-11 08:00:00 2017-07-14 30 F ## 191: Monitor64.txt 2017-07-11 08:00:00 2017-07-14 31 F ## 192: Monitor64.txt 2017-07-11 08:00:00 2017-07-14 32 F ## genotype replicate ## 1: A 1 ## 2: A 1 ## 3: A 1 ## 4: A 1 ## 5: A 1 ## --- ## 188: C 2 ## 189: C 2 ## 190: C 2 ## 191: C 2 ## 192: C 2 Each of the 192 animals (rows) is defined by a set of mandatory columns (metavariables): file – the data file (monitor) that it has been recorded in start_datetime – the date and time (YYYY-MM-DD HH:MM:SS) of the start of the experiment. Time will be considered ZT0, see note stop_datetime – the last time point of the experiment (time is optional) region_id – the channel ([1, 32]) For our experiment, we also defined custom columns: sex – M and F for male and female, respectively genotype – A, B or C (I just made up the names for the sake of simplicity) replicate – so we can analyse how replicates differ from one another Note that this format is very flexible and explicit. For instance, if we decided to do a third replicate, we would just need to add new rows. We could also add any condition we want as a new column (e.g. treatment, temperature, matting status and so on) Linking Linking is the one necessary step before loading the data. It allocates a unique identifier to each animal. It is very simple to link metadata: metadata &lt;- link_dam_metadata(metadata, result_dir = DATA_DIR) metadata As result_dir, we just use the directory where the data lives, which you decided when you extracted your data (DATA_DIR). Importantly, you do not need to cut the relevant parts of your DAM files (this is an error-prone step that should be avoided). In other words, no need to use the DAMFileScan utility or manipulate in any way the original data. You can keep all the data in one file per monitor. rethomics will use start and stop datetime to find the appropriate part directly from your metadata. Loading In order to work with the data the last step is to load it into a behavr structure. To do that simply use load_dam function (as shown below). This function will store all data in dt (or any other given name) dt &lt;- load_dam(metadata) summary(dt) ## behavr table with: ## 192 individuals ## 8 metavariables ## 2 variables ## 7.37472e+05 measurements ## 1 key (id) That is it, all our data is loaded in dt. Note on datetime ZT0 In the circadian and sleep field, we need to align our data to a reference time of the day. Typically, when the light (would) turn on (ZT0). In damr, the time part of the start_datetime is used as a circadian reference. For instance, if you specify, in your metadata file 2017-01-01 09:00:00, you imply that ZT0 is at 09:00:00. The time is looked-up in the DAM file, so it will be at on same time zone settings as the computer that recorded the data. Start and stop time When fetching some data, date and time are always inclusive. When only the date is specified: start time will be at 00:00:00 stop time will be at 23:59:59 For instance, start_date = 2017-01-01 and stop_date = 2017-01-01 retrieves all the data from the first of January 2017. Quality control Detecting anomalies Immediatly after loading your data, it is a good idea to visualise it, in order to detect anomalies or at least to be sure that everything looks ok. We can use ggetho for that, for example the following code will create an activity tile plot, useful to detect dead animals. library(ggetho) ## Loading required package: ggplot2 # I only show fisrt replicate ggetho(dt[xmv(replicate) == 1 ], aes(z=activity)) + stat_tile_etho() + stat_ld_annotations() Here, instead of ploting everything, I show how you can subset data according to metadata in order to display only replicate one (dt[xmv(replicate) == 1]). In practice, you could also plot everything. You can do a lot more with ggetho (see the visualisation chapter) What does this tile plot tell us? Each row is an animal (and is labelled with its corresponding id). Each column is a 30min window. The colour intensity indicates the activity. There are two things that we can immediatly notice: For most animals, the activity is rhythmic and synchronised with the light phase transisitions. Some animals are dead or missing. For instance take a look at channel 26 in Monitor64.txt. In other chapters, we will learn how to group individuals, visualise and compute statistics. How to exclude animals? We suggest to exclude animals a priori (e.g. because they died) by recording them as dead in the metadata. This way data is not modified or omited and can easily be recovered if needed. For instance, you can add a column status in your metadata file and put a default value such as &quot;OK&quot;. If an animal is to be removed, you can replace &quot;OK&quot; by a reason (e.g. &quot;dead&quot;, &quot;escaped&quot;, …). Then, you can load your data without those animals load_dam_data(metadata[status == &quot;OK&quot;], ...). This practice has the advantage of making it very transparent, why some individuals where excluded. Also, as stated before, it can easily be reversed. Apply functions when loading Finaly, we may want to apply a function on the data as it is loaded, in order to preprocess it, saving time. This pre-processing will annotate the data, i.e create new information (new columns) based on the original data. As an example, we can perform a sleep (bouts of immobility of 5 min or more), from our sleepr package (which you will have installed). library(sleepr) dt &lt;- load_dam(metadata, FUN = sleepr::sleep_dam_annotation) dt ## ## ==== METADATA ==== ## ## id file_info region_id ## &lt;fctr&gt; &lt;list&gt; &lt;int&gt; ## 1: 2017-07-01 08:00:00|Monitor11.txt|01 &lt;list&gt; 1 ## 2: 2017-07-01 08:00:00|Monitor11.txt|02 &lt;list&gt; 2 ## 3: 2017-07-01 08:00:00|Monitor11.txt|03 &lt;list&gt; 3 ## 4: 2017-07-01 08:00:00|Monitor11.txt|04 &lt;list&gt; 4 ## 5: 2017-07-01 08:00:00|Monitor11.txt|05 &lt;list&gt; 5 ## --- ## 188: 2017-07-11 08:00:00|Monitor64.txt|28 &lt;list&gt; 28 ## 189: 2017-07-11 08:00:00|Monitor64.txt|29 &lt;list&gt; 29 ## 190: 2017-07-11 08:00:00|Monitor64.txt|30 &lt;list&gt; 30 ## 191: 2017-07-11 08:00:00|Monitor64.txt|31 &lt;list&gt; 31 ## 192: 2017-07-11 08:00:00|Monitor64.txt|32 &lt;list&gt; 32 ## experiment_id start_datetime stop_datetime ## &lt;char&gt; &lt;POSc&gt; &lt;char&gt; ## 1: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 2: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 3: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 4: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 5: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## --- ## 188: 2017-07-11 08:00:00|Monitor64.txt 2017-07-11 08:00:00 2017-07-14 ## 189: 2017-07-11 08:00:00|Monitor64.txt 2017-07-11 08:00:00 2017-07-14 ## 190: 2017-07-11 08:00:00|Monitor64.txt 2017-07-11 08:00:00 2017-07-14 ## 191: 2017-07-11 08:00:00|Monitor64.txt 2017-07-11 08:00:00 2017-07-14 ## 192: 2017-07-11 08:00:00|Monitor64.txt 2017-07-11 08:00:00 2017-07-14 ## sex genotype replicate ## &lt;char&gt; &lt;char&gt; &lt;int&gt; ## 1: M A 1 ## 2: M A 1 ## 3: M A 1 ## 4: M A 1 ## 5: M A 1 ## --- ## 188: F C 2 ## 189: F C 2 ## 190: F C 2 ## 191: F C 2 ## 192: F C 2 ## ## ====== DATA ====== ## ## id t activity moving asleep ## &lt;fctr&gt; &lt;num&gt; &lt;int&gt; &lt;lgcl&gt; &lt;lgcl&gt; ## 1: 2017-07-01 08:00:00|Monitor11.txt|01 0 0 FALSE TRUE ## 2: 2017-07-01 08:00:00|Monitor11.txt|01 60 0 FALSE TRUE ## 3: 2017-07-01 08:00:00|Monitor11.txt|01 120 0 FALSE TRUE ## 4: 2017-07-01 08:00:00|Monitor11.txt|01 180 0 FALSE TRUE ## 5: 2017-07-01 08:00:00|Monitor11.txt|01 240 0 FALSE TRUE ## --- ## 737468: 2017-07-11 08:00:00|Monitor64.txt|32 230160 0 FALSE TRUE ## 737469: 2017-07-11 08:00:00|Monitor64.txt|32 230220 0 FALSE TRUE ## 737470: 2017-07-11 08:00:00|Monitor64.txt|32 230280 0 FALSE TRUE ## 737471: 2017-07-11 08:00:00|Monitor64.txt|32 230340 0 FALSE TRUE ## 737472: 2017-07-11 08:00:00|Monitor64.txt|32 230400 0 FALSE TRUE As you can see, we now have additional columns in the data. Next steps Visualise data with ggetho Sleep analysis with sleepr Circadian analysis with zeitgebr "],
["scopr.html", "Ethoscope data, in practice Aims Prerequisites Background Getting the data From experiment design to metadata Linking Loading Quality control And then? Next steps", " Ethoscope data, in practice Large data Example of an ethoscope experiment. Twenty flies per ethoscope are either treated or not. Positions are randomised. Two machines are used in parallel, and two replicates are performed at another time. In total, 80 animals were present in this experiment. Four .db files have been recorded. Tracked animals are also subsequently scored for another phenotype (gene expression level) Aims In this practical chapter, we will use a real experiment to learn how to: Translate your experiment design into a metadata file Use this metadata file to load some data Set the circadian reference (ZT0) Optimise your code to save time and RAM Assess graphically the quality of the data Prerequisites You are familiar with the Ethoscope Platform Ensure you have read about the rethomics workflow and metadata Ensure you have installed behavr, scopr and ggetho packages: library(devtools) install_github(&quot;rethomics/behavr&quot;) install_github(&quot;rethomics/scopr&quot;) install_github(&quot;rethomics/ggetho&quot;) Background The Ethoscope Platform is a versatile and modular behavioural system to sudy behaviour of small animals. You can read more about it in our PLOS Biology publication. Although you can do much more with them, ethoscope are primarily designed to study sleep and circadian rhythms in Drosophila. Therefore, this tutorial targets mainly users in this context. Ethoscopes typically generate hundreds of megabytes per machine per week. The platform is designed so that many devices run an parallel, and the resulting data is eventually centralised for all users. Metadata tables are the best way to both keep track of your expriments and fetch the relevant data. Getting the data Extract the zipped data Since ethoscopes generate large amount of data, compared to DAM, we will work only with a few animals. A zip containing data for this tutorial is available on zenodo. Start by downloading and extracting the zip somewhere in your computer. Then, store this location as a variable. For example, adapt the path here: DATA_DIR &lt;- &quot;C://Where/My/Zip/Has/been/extracted Check that all the files live there: list.files(DATA_DIR) ## [1] &quot;ethoscope_results&quot; &quot;ethoscope_tutorial.zip&quot; ## [3] &quot;metadata.csv&quot; We have: The metadata file that I have prepared for you An ethoscope_results directory Note on the data structure It is informative to take a look at the latter. Files are organised as &lt;machine_id&gt;/&lt;machine_name&gt;/&lt;datetime&gt;/&lt;file&gt;: ethoscope_results ├── 008d6ba04e534be486069c3db7b10827 │ └── ETHOSCOPE_008 │ └── 2016-07-29_14-57-35 │ └── 2016-07-29_14-57-35_008d6ba04e534be486069c3db7b10827.db ├── 009d6ba04e534be486069c3db7b10827 │ └── ETHOSCOPE_009 │ └── 2016-07-22_16-43-29 │ └── 2016-07-22_16-43-29_009d6ba04e534be486069c3db7b10827.db └── 011d6ba04e534be486069c3db7b10827 └── ETHOSCOPE_011 ├── 2016-07-22_16-41-21 │ └── 2016-07-22_16-41-21_011d6ba04e534be486069c3db7b10827.db └── 2016-07-29_14-59-49 └── 2016-07-29_14-59-49_011d6ba04e534be486069c3db7b10827.db Tracking data are saved in .db files. Everytime an ethoscope is started, one new .db file is created. All animals present in the same machine at the same time will have their data saved in the same .db file. Set your working directory For this tutorial, we will just set our working directory to DATA_DIR: setwd(DATA_DIR) From experiment design to metadata Our toy experiment The data I gave you is from a real experiment, but for the sake of the tutorial, I have made up a story around it. It goes like that: We are interested in the effect of a drug, let’s call it the “mystery drug”, on sleep and activity in fruit flies We supect this drug acts indirectly by inhibiting the expression of a (mystery) gene To address these two questions, we have delivered a drug (yes or no) to fruit flies, and randomised their position in two ethoscopes. We then acquiered individual data (e.g. position and activity) about twice a second for several days. We performed two replicates, one week appart. In addition, in the end of the experiment, we also collected flies and measured the relative expression of our candidate gene. Ultimatly, we would like to: Analyse the effect of the drug on sleep Study the correlations between the activity (or the drug) and the level of transcript Our experimental design Metadata It is crucial that you have read metadata chapter to understand this part. Our goal is to encode our whole experiment in a single file in which: each row is an individual each column is a metavariable Luckily for you, I have already put this file together for you as metadata.csv! Lets have a look at it (you can use R, excel or whatever you want). If you are using R, type this commands: library(scopr) metadata &lt;- fread(&quot;metadata.csv&quot;) metadata ## machine_name date region_id treatment replicate ## 1: ETHOSCOPE_011 2016-07-22 1 yes 1 ## 2: ETHOSCOPE_011 2016-07-22 2 no 1 ## 3: ETHOSCOPE_011 2016-07-22 3 yes 1 ## 4: ETHOSCOPE_011 2016-07-22 4 yes 1 ## 5: ETHOSCOPE_011 2016-07-22 5 yes 1 ## --- ## 76: ETHOSCOPE_011 2016-07-29 16 yes 2 ## 77: ETHOSCOPE_011 2016-07-29 17 no 2 ## 78: ETHOSCOPE_011 2016-07-29 18 no 2 ## 79: ETHOSCOPE_011 2016-07-29 19 no 2 ## 80: ETHOSCOPE_011 2016-07-29 20 yes 2 ## expression_level ## 1: 3.28 ## 2: 2.16 ## 3: 2.11 ## 4: 3.54 ## 5: 3.98 ## --- ## 76: 4.76 ## 77: 2.66 ## 78: 4.35 ## 79: 1.06 ## 80: 5.21 Each of the 80 animals (rows) is defined by two mandatory columns (metavariables): machine_name – the name of the device used (e.g. &quot;ETHOSCOPE_001&quot;) date – the date and time (YYYY-MM-DD) of the start of the experiment region_id – The tube (region of interest) in which an animal was tracked ([1,20]) We defined also custom columns (i.e. metavariables). Note that you could define many, with arbitrary names: treatment – “yes” or “no”, whether the drug was administered replicate – 1 or 2, firth or second replicate expression_level – a continuous number [0, +Inf], how much a candidate gene is transcribed Importantly, you should be able to understand the experiment, and describe each animal, from the metadata file. Linking Linking is the one necessary step before loading the data. It allocates a unique identifier to each animal. In addition, it finds the file needed to load its data. Local data If your data is already in your computer (as it is now sice you have downloaded it manually), you can simply link it like: metadata &lt;- link_ethoscope_metadata(metadata, result_dir = &quot;ethoscope_results&quot;) print(metadata) ## id file_info machine_name ## 1: 2016-07-22_16-41-21_011d6b|01 &lt;list&gt; ETHOSCOPE_011 ## 2: 2016-07-22_16-41-21_011d6b|02 &lt;list&gt; ETHOSCOPE_011 ## 3: 2016-07-22_16-41-21_011d6b|03 &lt;list&gt; ETHOSCOPE_011 ## 4: 2016-07-22_16-41-21_011d6b|04 &lt;list&gt; ETHOSCOPE_011 ## 5: 2016-07-22_16-41-21_011d6b|05 &lt;list&gt; ETHOSCOPE_011 ## --- ## 76: 2016-07-29_14-59-49_011d6b|16 &lt;list&gt; ETHOSCOPE_011 ## 77: 2016-07-29_14-59-49_011d6b|17 &lt;list&gt; ETHOSCOPE_011 ## 78: 2016-07-29_14-59-49_011d6b|18 &lt;list&gt; ETHOSCOPE_011 ## 79: 2016-07-29_14-59-49_011d6b|19 &lt;list&gt; ETHOSCOPE_011 ## 80: 2016-07-29_14-59-49_011d6b|20 &lt;list&gt; ETHOSCOPE_011 ## machine_id datetime region_id ## 1: 011d6ba04e534be486069c3db7b10827 2016-07-22 16:41:21 1 ## 2: 011d6ba04e534be486069c3db7b10827 2016-07-22 16:41:21 2 ## 3: 011d6ba04e534be486069c3db7b10827 2016-07-22 16:41:21 3 ## 4: 011d6ba04e534be486069c3db7b10827 2016-07-22 16:41:21 4 ## 5: 011d6ba04e534be486069c3db7b10827 2016-07-22 16:41:21 5 ## --- ## 76: 011d6ba04e534be486069c3db7b10827 2016-07-29 14:59:49 16 ## 77: 011d6ba04e534be486069c3db7b10827 2016-07-29 14:59:49 17 ## 78: 011d6ba04e534be486069c3db7b10827 2016-07-29 14:59:49 18 ## 79: 011d6ba04e534be486069c3db7b10827 2016-07-29 14:59:49 19 ## 80: 011d6ba04e534be486069c3db7b10827 2016-07-29 14:59:49 20 ## treatment replicate expression_level ## 1: yes 1 3.28 ## 2: no 1 2.16 ## 3: yes 1 2.11 ## 4: yes 1 3.54 ## 5: yes 1 3.98 ## --- ## 76: yes 2 4.76 ## 77: no 2 2.66 ## 78: no 2 4.35 ## 79: no 2 1.06 ## 80: yes 2 5.21 Remote Data In real life situations, when several experimenters are working in parallel, Users should not: Have to download all the data from all users at all time (very inefficient) Download data by hand from a server (error prone and time consuming) Instead, it will be more common that all the data is stored on an FTP server (i.e. a network drive). scopr allows us to look up in the metadata and download, incrementally, only the files needed. For more information about how to set up network backups check the ethocope documentation. In this context, we will be using link_ethoscope_metadata_remote. It works almost exactly like link_ethoscope_metadata, but takes a remote_dir argument. remote_dir is generally the address to an FTP directory. This is just an example that you will have to adapt to your own situation: REMOTE_DIR &lt;- &quot;ftp://my-share-drive.com/where/the-data/lives&quot; RESULT_DIR &lt;- &quot;/where/to/save/the/data&quot; metadata &lt;- link_ethoscope_metadata_remote(metadata, remote_dir = REMOTE_DIR, result_dir = RESULT_DIR, verbose = TRUE) Note that, as long as you have an internet connection, you can use this function to link your metadata. It will not download data files everytime (unless new data is available upstream). Loading The core fucntion of scopr is load_ethoscope(). It is quite flexible and we will show here just a few examples of how to use it. Do have a look at the documentation (e.g. by running ?load_ethoscope), if you want to know more. Raw data (default) The simplest thing one can do is to load all tracking data. However, this comes with several caveats: A lot (gigabytes) of data will be needed You will probably have to process the data immediatly after in order to extract biological meaning Here, we will just load data from animals in region 1 (metadata[region_id == 1]). As it is could otherwise be too much data: metadata_subset &lt;- metadata[region_id == 1] dt &lt;- load_ethoscope(metadata_subset, verbose=FALSE) summary(dt) ## behavr table with: ## 4 individuals ## 8 metavariables ## 8 variables ## 3.004648e+06 measurements ## 1 key (id) Note that I set verbose to FALSE. This is to avoid printing progress. You may want to set it to TRUE (the default), so you can check how fast data loading is happening. It is also good practice to print your resutling behavr table (dt in this case): print(dt) ## ## ==== METADATA ==== ## ## id file_info machine_name ## &lt;fctr&gt; &lt;list&gt; &lt;char&gt; ## 1: 2016-07-22_16-41-21_011d6b|01 &lt;list&gt; ETHOSCOPE_011 ## 2: 2016-07-22_16-43-29_009d6b|01 &lt;list&gt; ETHOSCOPE_009 ## 3: 2016-07-29_14-57-35_008d6b|01 &lt;list&gt; ETHOSCOPE_008 ## 4: 2016-07-29_14-59-49_011d6b|01 &lt;list&gt; ETHOSCOPE_011 ## machine_id datetime region_id ## &lt;char&gt; &lt;POSc&gt; &lt;int&gt; ## 1: 011d6ba04e534be486069c3db7b10827 2016-07-22 16:41:21 1 ## 2: 009d6ba04e534be486069c3db7b10827 2016-07-22 16:43:29 1 ## 3: 008d6ba04e534be486069c3db7b10827 2016-07-29 14:57:35 1 ## 4: 011d6ba04e534be486069c3db7b10827 2016-07-29 14:59:49 1 ## treatment replicate expression_level ## &lt;char&gt; &lt;int&gt; &lt;num&gt; ## 1: yes 1 3.28 ## 2: yes 1 4.28 ## 3: no 2 2.52 ## 4: yes 2 6.04 ## ## ====== DATA ====== ## ## id t x y ## &lt;fctr&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; ## 1: 2016-07-22_16-41-21_011d6b|01 37.561 0.4003591 0.05565530 ## 2: 2016-07-22_16-41-21_011d6b|01 38.170 0.4021544 0.05565530 ## 3: 2016-07-22_16-41-21_011d6b|01 38.760 0.4021544 0.05565530 ## 4: 2016-07-22_16-41-21_011d6b|01 39.316 0.4039497 0.05385996 ## 5: 2016-07-22_16-41-21_011d6b|01 39.934 0.4021544 0.05385996 ## --- ## 3004644: 2016-07-29_14-59-49_011d6b|01 583498.570 0.8432432 0.04684685 ## 3004645: 2016-07-29_14-59-49_011d6b|01 583499.395 0.8306306 0.05585586 ## 3004646: 2016-07-29_14-59-49_011d6b|01 583500.285 0.8270270 0.04684685 ## 3004647: 2016-07-29_14-59-49_011d6b|01 583501.175 0.8432432 0.04504505 ## 3004648: 2016-07-29_14-59-49_011d6b|01 583502.037 0.8396396 0.05045045 ## w h phi xy_dist_log10x1000 has_interacted ## &lt;num&gt; &lt;num&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1: 0.05206463 0.02154399 152 -392 0 ## 2: 0.04308797 0.01615799 159 -2453 0 ## 3: 0.04129264 0.01795332 162 -2633 0 ## 4: 0.03949731 0.01615799 153 -2513 0 ## 5: 0.04308797 0.01795332 163 -2666 0 ## --- ## 3004644: 0.03783784 0.02522523 0 -2045 0 ## 3004645: 0.03963964 0.02342342 172 -1756 0 ## 3004646: 0.04324324 0.02522523 10 -1959 0 ## 3004647: 0.03963964 0.02522523 164 -1785 0 ## 3004648: 0.03783784 0.02702703 169 -2097 0 It shows us the metadata as well as the first few and last few lines of actual data. In the data, there are columns such as x and y that record position, but also others that may not make immediate sense to you. In practice, loading raw data is rare for long experiments, and I would recommend doing it only for prototyping and such. Preprocessing As dissussed before, it is inefficient and not always possible to load all raw data for hundreads of animals. Instead, we can preprocess data on the go. In the context of activity and sleep analysis, we can quantify activity in windows of 10s (techinical details here). This is implemented in the function sleep_annotation of our sleepr package. Ensure you have installed sleepr as well (devtools::install_github(&quot;rethomics/sleepr&quot;)). When running this function: The activity in each 10s of data will be scored The position will be kept Sleep will be scored according to the “five minute rule” To apply a function to all individual data, as they are loaded, we can use the FUN argument: dt &lt;- load_ethoscope(metadata, FUN = sleepr::sleep_annotation, verbose=FALSE) summary(dt) ## behavr table with: ## 80 individuals ## 8 metavariables ## 8 variables ## 4.641275e+06 measurements ## 1 key (id) Again, we have a look at the resulting table: print(dt) ## ## ==== METADATA ==== ## ## id file_info machine_name ## &lt;fctr&gt; &lt;list&gt; &lt;char&gt; ## 1: 2016-07-22_16-41-21_011d6b|01 &lt;list&gt; ETHOSCOPE_011 ## 2: 2016-07-22_16-41-21_011d6b|02 &lt;list&gt; ETHOSCOPE_011 ## 3: 2016-07-22_16-41-21_011d6b|03 &lt;list&gt; ETHOSCOPE_011 ## 4: 2016-07-22_16-41-21_011d6b|04 &lt;list&gt; ETHOSCOPE_011 ## 5: 2016-07-22_16-41-21_011d6b|05 &lt;list&gt; ETHOSCOPE_011 ## --- ## 76: 2016-07-29_14-59-49_011d6b|16 &lt;list&gt; ETHOSCOPE_011 ## 77: 2016-07-29_14-59-49_011d6b|17 &lt;list&gt; ETHOSCOPE_011 ## 78: 2016-07-29_14-59-49_011d6b|18 &lt;list&gt; ETHOSCOPE_011 ## 79: 2016-07-29_14-59-49_011d6b|19 &lt;list&gt; ETHOSCOPE_011 ## 80: 2016-07-29_14-59-49_011d6b|20 &lt;list&gt; ETHOSCOPE_011 ## machine_id datetime region_id ## &lt;char&gt; &lt;POSc&gt; &lt;int&gt; ## 1: 011d6ba04e534be486069c3db7b10827 2016-07-22 16:41:21 1 ## 2: 011d6ba04e534be486069c3db7b10827 2016-07-22 16:41:21 2 ## 3: 011d6ba04e534be486069c3db7b10827 2016-07-22 16:41:21 3 ## 4: 011d6ba04e534be486069c3db7b10827 2016-07-22 16:41:21 4 ## 5: 011d6ba04e534be486069c3db7b10827 2016-07-22 16:41:21 5 ## --- ## 76: 011d6ba04e534be486069c3db7b10827 2016-07-29 14:59:49 16 ## 77: 011d6ba04e534be486069c3db7b10827 2016-07-29 14:59:49 17 ## 78: 011d6ba04e534be486069c3db7b10827 2016-07-29 14:59:49 18 ## 79: 011d6ba04e534be486069c3db7b10827 2016-07-29 14:59:49 19 ## 80: 011d6ba04e534be486069c3db7b10827 2016-07-29 14:59:49 20 ## treatment replicate expression_level ## &lt;char&gt; &lt;int&gt; &lt;num&gt; ## 1: yes 1 3.28 ## 2: no 1 2.16 ## 3: yes 1 2.11 ## 4: yes 1 3.54 ## 5: yes 1 3.98 ## --- ## 76: yes 2 4.76 ## 77: no 2 2.66 ## 78: no 2 4.35 ## 79: no 2 1.06 ## 80: yes 2 5.21 ## ## ====== DATA ====== ## ## id t x max_velocity ## &lt;fctr&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; ## 1: 2016-07-22_16-41-21_011d6b|01 40 0.4021544 17.4533479 ## 2: 2016-07-22_16-41-21_011d6b|01 50 0.6014363 13.0280299 ## 3: 2016-07-22_16-41-21_011d6b|01 60 0.7612208 4.7849648 ## 4: 2016-07-22_16-41-21_011d6b|01 70 0.7737882 0.8627376 ## 5: 2016-07-22_16-41-21_011d6b|01 80 0.7719928 3.5227250 ## --- ## 4641271: 2016-07-29_14-59-49_011d6b|20 583450 0.4440433 6.8529687 ## 4641272: 2016-07-29_14-59-49_011d6b|20 583460 0.3989170 19.8097386 ## 4641273: 2016-07-29_14-59-49_011d6b|20 583470 0.1480144 3.8714954 ## 4641274: 2016-07-29_14-59-49_011d6b|20 583480 0.1660650 23.0610324 ## 4641275: 2016-07-29_14-59-49_011d6b|20 583490 0.4151625 29.4359967 ## interactions beam_crosses moving asleep is_interpolated ## &lt;int&gt; &lt;int&gt; &lt;lgcl&gt; &lt;lgcl&gt; &lt;lgcl&gt; ## 1: 0 1 TRUE FALSE FALSE ## 2: 0 0 TRUE FALSE FALSE ## 3: 0 0 TRUE FALSE FALSE ## 4: 0 0 FALSE FALSE FALSE ## 5: 0 0 TRUE FALSE FALSE ## --- ## 4641271: 0 0 TRUE FALSE FALSE ## 4641272: 0 0 TRUE FALSE FALSE ## 4641273: 0 0 TRUE FALSE FALSE ## 4641274: 0 0 TRUE FALSE FALSE ## 4641275: 0 1 TRUE FALSE FALSE We have several variables (see in the ===DATA=== section of the output). The important ones are: t – the time, in s, sample every 10s x – the position in the tube (0 = left, 1=right) beam_cross – how many times the midline was crossed within the 10s window moving – whether any movement was detected in this time window asleep – whether the animal is asleep during this 10s (5min rule) Note that you can use other functions than sleep_annotation (this is just an example) and even define your own! ZT0 By default, the time is expressed relative to the onset of the experiment. In other words, \\(t_0\\) is when you click on the “start tracking” button. In the context of sleep, we care more about time within a day. Therefore, we express time relative to ZT0 (i.e the hour of the day when the L phase starts). In our lab, it is at 09:00:00, GMT. In load_ethoscope, we translate that by using the argument reference_hour=9.0: dt &lt;- load_ethoscope(metadata, reference_hour=9.0, FUN = sleepr::sleep_annotation, verbose=FALSE) Cache directory One issue with loading ethoscope data is thta it is relatively slow. For instance, according to our computer, you may take half an hour to load data from 500 animals \\(times\\) 1 week. In your daily life, you will often need to close R and open it again later to analyse things differently, or simply change simple parameters on some figures. You really don’t want to spend too long reloading data this sort of situation. To address this issue, load_ethoscope comes with a caching option. When turned on, whenever you load any data, it stores a snapshot on the disk (in a directory you pick). Then, the next time you load the data, loading happends directly from the snapshot. This way you will load data one or two orders of magnitude faster! The first time, everything will take as long: system.time( dt &lt;- load_ethoscope(metadata, reference_hour=9.0, FUN = sleepr::sleep_annotation, cache = &quot;ethoscope_cache&quot;, verbose=FALSE) ) ## user system elapsed ## 142.126 2.228 144.357 However, the next time, we do the same thing way faster: system.time( dt &lt;- load_ethoscope(metadata, reference_hour=9.0, FUN = sleepr::sleep_annotation, cache = &quot;ethoscope_cache&quot;, verbose=FALSE) ) ## user system elapsed ## 1.723 0.088 1.811 Here, we set cache to &quot;ethoscope_cache&quot;. This creates a new directory called &quot;ethoscope_cache&quot; (you can pick the name and location you want). If you to do a bit of tidying up, you can remove it without risk (it will just take time next time you load your data). Quality control One way to check everything is in order with our data is to visualise it with a tile plot. For instance, here, the variable asleep, for each animal (rows in the plot) over time (columns): library(ggetho) ggetho(dt, aes(z=asleep)) + stat_tile_etho() + stat_ld_annotations() This allows us to spot possible outliers/missing data/artefacts and try to understand what to do whith them. More about tiles plots in the visualisation chapter. And then? Just a teaser to show you how we can already work on our biological question at this stage, Sleep amount vs treatment We can start to see that treatment increases sleep: library(ggetho) ggetho(dt, aes(y=asleep, colour=treatment), time_wrap = hours(24)) + stat_pop_etho() + stat_ld_annotations() Total sleep vs expression level First we summarise the total proportion of time spent sleeping for each animal (i.e. by id). summary_dt &lt;- dt[, .(sleep_fraction = mean(asleep)), by=id] Then we rejoin this summary to metadata: summary_dt &lt;- rejoin(summary_dt) summary_dt ## id file_info machine_name ## 1: 2016-07-22_16-41-21_011d6b|01 &lt;list&gt; ETHOSCOPE_011 ## 2: 2016-07-22_16-41-21_011d6b|02 &lt;list&gt; ETHOSCOPE_011 ## 3: 2016-07-22_16-41-21_011d6b|03 &lt;list&gt; ETHOSCOPE_011 ## 4: 2016-07-22_16-41-21_011d6b|04 &lt;list&gt; ETHOSCOPE_011 ## 5: 2016-07-22_16-41-21_011d6b|05 &lt;list&gt; ETHOSCOPE_011 ## --- ## 76: 2016-07-29_14-59-49_011d6b|16 &lt;list&gt; ETHOSCOPE_011 ## 77: 2016-07-29_14-59-49_011d6b|17 &lt;list&gt; ETHOSCOPE_011 ## 78: 2016-07-29_14-59-49_011d6b|18 &lt;list&gt; ETHOSCOPE_011 ## 79: 2016-07-29_14-59-49_011d6b|19 &lt;list&gt; ETHOSCOPE_011 ## 80: 2016-07-29_14-59-49_011d6b|20 &lt;list&gt; ETHOSCOPE_011 ## machine_id datetime region_id ## 1: 011d6ba04e534be486069c3db7b10827 2016-07-22 16:41:21 1 ## 2: 011d6ba04e534be486069c3db7b10827 2016-07-22 16:41:21 2 ## 3: 011d6ba04e534be486069c3db7b10827 2016-07-22 16:41:21 3 ## 4: 011d6ba04e534be486069c3db7b10827 2016-07-22 16:41:21 4 ## 5: 011d6ba04e534be486069c3db7b10827 2016-07-22 16:41:21 5 ## --- ## 76: 011d6ba04e534be486069c3db7b10827 2016-07-29 14:59:49 16 ## 77: 011d6ba04e534be486069c3db7b10827 2016-07-29 14:59:49 17 ## 78: 011d6ba04e534be486069c3db7b10827 2016-07-29 14:59:49 18 ## 79: 011d6ba04e534be486069c3db7b10827 2016-07-29 14:59:49 19 ## 80: 011d6ba04e534be486069c3db7b10827 2016-07-29 14:59:49 20 ## treatment replicate expression_level sleep_fraction ## 1: yes 1 3.28 0.5716289 ## 2: no 1 2.16 0.3962078 ## 3: yes 1 2.11 0.5480788 ## 4: yes 1 3.54 0.5802635 ## 5: yes 1 3.98 0.5451192 ## --- ## 76: yes 2 4.76 0.4489939 ## 77: no 2 2.66 0.4063041 ## 78: no 2 4.35 0.2806088 ## 79: no 2 1.06 0.3703443 ## 80: yes 2 5.21 0.3803006 This is a standard data.table (data.frame), so we can use it for some regular R magic. We can show the how sleep fraction and expression level are decorrelated ggplot(summary_dt, aes(x=expression_level, y=sleep_fraction, colour = treatment)) + geom_point() + geom_smooth(method=&quot;lm&quot;) Next steps Visualise data with ggetho Sleep analysis with sleepr Circadian analysis with zeitgebr "],
["ggetho.html", "Visualisation with ggetho Aims Prerequisites Lessons from ggplot Some behavioural data The ggetho() function Tile plots Population plots Wrapping data Double-plotted actograms Faceting by ID LD annotations Coordinate and scales Periodograms Spectrograms Take home message Next steps", " Visualisation with ggetho Aims In this practical chapter, we will generate toy data to learn how to: Express a question as a relationship beween variables Use tile plots to show individual data Make population plots Wrap data around circadian time Make double-plotted actograms Annotate plot with light and dark phases Use ggplot tools (facets, scales) to enhance plots Plot average and individual periodograms Prerequisites Some familiarity with ggplot Ensure you have installed behavrand ggetho packages: library(devtools) install_github(&quot;rethmics/behavr&quot;) install_github(&quot;rethmics/ggetho&quot;) Lessons from ggplot In the previous tutorials, we have used ggetho to visualise out behavioural data. This section will explain further how this package can be used to produce flexible plots and how it integrates with ggplot2. ggplot2 is one of the most popular visualisation tools and an unavoidable R package. It implements the powerful concepts of the “Grammar of graphics”. The package ggetho, which we discuss here, extends ggplot for the specific case of behavioural analysis. At this stage, you really want to have some familiarity with ggplot2 so you understand its logic. You will find a fair numbers of videos and books online. Some behavioural data In this section, we will simulate toy behavioural data. For that, we start by making some arbitrary metadata. Here, we have 40 animals, condition “A” vs “B”, and sex, male (“M”) or female (“F”). library(ggetho) metadata &lt;- data.table(id=sprintf(&quot;toy_experiment|%02d&quot; , 1:40), region_id=1:40, condition=c(&quot;A&quot;,&quot;B&quot;), sex=c(&quot;M&quot;,&quot;M&quot;, &quot;F&quot;, &quot;F&quot;)) head(metadata) ## id region_id condition sex ## 1: toy_experiment|01 1 A M ## 2: toy_experiment|02 2 B M ## 3: toy_experiment|03 3 A F ## 4: toy_experiment|04 4 B F ## 5: toy_experiment|05 5 A M ## 6: toy_experiment|06 6 B M dt &lt;- toy_activity_data(metadata, seed=107) Now, we have a behavr object, dt: summary(dt) ## behavr table with: ## 40 individuals ## 3 metavariables ## 3 variables ## 1.72804e+06 measurements ## 1 key (id) This data is stored in a behavr table. It has a column moving that that tells us whether an the animal id is moving at a time t. The ggetho() function ggetho() is the core function. It expresses the relationship between variables. In this respect, it works very much like ggplot(), but it also pre-processes the data. It is important to understand the difference between ggplot() and ggetho(). ggplot() works with data frames (or data tables), and does not preprocess the data. ggetho() is only a layer on top of ggplot(). It works exclusively with behavr tables and does preprocess data before calling ggplot(). ggetho() does return ggplot a object, therefore, layers available in ggplot2 can be used natively on top of ggetho. Let’s work with an example. Say, we would like: The proportion of time spent moving, on the y axis Versus time, on the x axis We could write: pl &lt;- ggetho(dt, aes(x=t, y=moving)) pl This generates an empty plot this is normal because we have, so far, no layer. We will see some layers very soon! The role of ggetho is to express a relationship between variables and to compute a summary, over a certain time window, of a variable of interest for each individual. Importantly, you decide which variable you want to plot. For instance, you could be interested in things like the number (sum) of beam crosses or the average position. Tile plots Per individual One of the most interesting layers is stat_tile_etho. It shows the variable of interest in the (colour) z axis. The y axis is discrete (generally the id), that is one row per individual. The x axis is time (by default, summerised, by ggetho, over 30 minutes). So, if we want to show the proportion of time spent moving over time for each individual (id): pl &lt;- ggetho(dt, aes(x=t, y=id, z=moving)) + stat_tile_etho() pl By defaut, each pixel is the mean (summary_FUN = mean, in ggetho), over 30 min (summary_time_window = mins(30), in ggetho()). Also, note that the default is x=t and y=id, so we could just obtain exactly the same with ggetho(dt, aes(z=moving)) + stat_tile_etho(). Sorted individual Sometimes, we want to sort individuals based on a metavariable (discrete or continuous). For instance let us compute the overall average fraction of time spent moving, add it to the metadata, to then sort individuals from low to high movers: First, we add a new metavariable (mean_moving): # the average time spent moving per 1000 (rounded) mean_mov_dt &lt;- dt[, .(mean_moving = round(mean(moving) * 1000)), by=id] # join curent meta and the summary table new_meta &lt;- dt[mean_mov_dt, meta=T] # set new metadata setmeta(dt, new_meta) head(dt[meta=T]) ## id region_id condition sex mean_moving ## 1: toy_experiment|01 1 A M 138 ## 2: toy_experiment|02 2 B M 195 ## 3: toy_experiment|03 3 A F 90 ## 4: toy_experiment|04 4 B F 118 ## 5: toy_experiment|05 5 A M 123 ## 6: toy_experiment|06 6 B M 203 Proportion of time moving (mean(moving)) will be a number between zero and one, with sometimes many decimals, which makes it hard to read. A more intuitive way to express it is as a rounded per mille (‰). I prefer it to per cent as we have more resolution (i.e. less ties) For instance, 0.0113333 would be expressed as 11. Now, we can express a new relationship where we show the interaction between our custom variable and id, on the y axis: pl &lt;- ggetho(dt, aes(x=t, y=interaction(id, mean_moving, sep = &quot; : &quot;), z=moving)) + stat_tile_etho() pl Since we use &quot; : &quot; as a separator, we have, on the y axis, names as &lt;id&gt; : &lt;mean_sleep&gt;. You can extend this concept to sort also by males vs females: pl &lt;- ggetho(dt, aes(x=t, y=interaction(id, mean_moving, sex, sep = &quot; : &quot;), z=moving)) + stat_tile_etho() pl Group averages Sometimes, we also want to aggregate individuals per group. For instance, males average vs females average. This can be done by changing the y axis. Previously, we used id, which made one row per individual. Instead, if we use a grouping variable like sex, we will plot one row per value of sex (i.e. two rows, one for males, one for females). In other words, we replace id by sex on the y axis: pl &lt;- ggetho(dt, aes(x=t, y=sex, z=moving)) + stat_tile_etho() pl In this context, every row is not an individual any more, but a population. The method argument of stat_tile_etho() allows you to use other aggregates (median, max, min, …). Bar tiles The bar_tile is a variant of our tile plot. Instead of colour intensity, it shows our z variable by the height of the tiles. You can use it just by replacing stat_tile_etho by stat_bar_tile_etho: pl &lt;- ggetho(dt, aes(x=t, z=moving)) + stat_bar_tile_etho() pl Population plots One population The problem with representing a variable on a colour axis is that it is not perceptually comparable, and we cannot make error bars. When the number of groups is not too high, it makes sense to show the variable of interest on the y axis, and then draw lines between consecutive points. For this, we can use the stat_pop_etho() function: pl &lt;- ggetho(dt, aes(x=t, y=moving)) + stat_pop_etho() pl By defaut, the local average and error bars are computed from the mean as standard errors (method = mean_se). You can compute other types of error bars e.g. bootstrap (method = mean_cl_boot). Several populations Often, we want to compare population with respect to a variable. There are different ways to split populations. We can, for instance, use a different colour line for different groups: pl &lt;- ggetho(dt, aes(x=t, y=moving, colour=sex)) + stat_pop_etho() pl Another way, is to use ggplot’s faceting system: pl &lt;- ggetho(dt, aes(x=t, y=moving)) + stat_pop_etho() + facet_grid(sex ~ .) pl Of course, you can combine both when you have more than one relevant metavariable: pl &lt;- ggetho(dt, aes(x=t, y=moving, colour = sex)) + stat_pop_etho() + facet_grid( condition ~ .) pl Wrapping data When behaviours are periodic, we sometimes want to average our variable at the same time over consecutive days. In ggetho, we call that time wrapping. It can be done simply with the time_wrap argument. It will work the same for population or tile plots: pl &lt;- ggetho(dt, aes(x=t, y=moving), time_wrap = hours(24)) + stat_pop_etho() pl Note that you do not have to wrap specifically over 24h, you could work over different periods. If you are interested in events that happen between the end and the start of the wrapping period (e.g. at ZT24). You may want to wrap time with an “offset”. That is a phase shift. For instance, if we want to have ZT06 in the middle of our graph, we use an offset of +6h: pl &lt;- ggetho(dt, aes(x=t, y=moving), time_wrap = hours(24), time_offset = hours(6)) + stat_pop_etho() pl As you can see, it gives us a nice visualisation of the “activity peaks”. Double-plotted actograms When analysing periodic behaviour, it makes sense to use a so called double-plotted actogram. This is very useful to understand periodicity of behaviours. This means data is plotted twice, in a staggered manner: row1 [day 1, day2] row1 [day 2, day3] row1 [day 3, day4] To do that, we can set the multiplot argument of ggetho to 2 (3 would do a “tripple-plotted” actogram). This averages the whole population: pl &lt;- ggetho(dt, aes(x=t, z=moving), multiplot = 2) + stat_bar_tile_etho() pl In practice, we genrally want to do that for one specific individual (see next section to do that automatically): pl &lt;- ggetho(dt[id==&quot;toy_experiment|01&quot;], aes(x=t, z=moving), multiplot = 2) + stat_bar_tile_etho() pl One thing you can do is change the length of the period. For instance 25h instead of 24h: pl &lt;- ggetho(dt[id==&quot;toy_experiment|01&quot;], aes(x=t, z=moving), multiplot = 2, multiplot_period = hours(25) # this is the important part ) + stat_bar_tile_etho() pl Keep in mind that you can use the tile representation if you prefer it: pl &lt;- ggetho(dt[id==&quot;toy_experiment|01&quot;], aes(x=t, z=moving), multiplot = 2 ) + stat_tile_etho() # tile here pl Faceting by ID When multiplotting, it is difficult to represent individuals (since both y and x axis are used). Default The best way to systematically represent all of them is to use facetting, which is a ggplot feature. Since id represents unique individuals, each facet (sub-rectangle) is one individual: pl &lt;- ggetho(dt, aes(x=t, z=moving), multiplot = 2 ) + stat_bar_tile_etho() + facet_wrap( ~ id) pl Custom labeller Sometimes, the id variable will be very long, you can use the id_labeller to make things clearer: pl &lt;- ggetho(dt, aes(x=t, z=moving), multiplot = 2 ) + stat_bar_tile_etho() + facet_wrap( ~ id, labeller = &quot;id_labeller&quot;) pl Numerical labelling Even with the trick above, ids may become unreadable when plotting many individuals. What we can can do in this case is to use numbers instead. First, we create a new metavariable named, for instance, uid. It will be one unique number per individual: dt[, uid := 1:.N, meta=T] print(dt[meta=T]) ## id region_id condition sex mean_moving uid ## 1: toy_experiment|01 1 A M 138 1 ## 2: toy_experiment|02 2 B M 195 2 ## 3: toy_experiment|03 3 A F 90 3 ## 4: toy_experiment|04 4 B F 118 4 ## 5: toy_experiment|05 5 A M 123 5 ## --- ## 36: toy_experiment|36 36 B F 105 36 ## 37: toy_experiment|37 37 A M 228 37 ## 38: toy_experiment|38 38 B M 78 38 ## 39: toy_experiment|39 39 A F 59 39 ## 40: toy_experiment|40 40 B F 84 40 Then, we can use uid instead of id: pl &lt;- ggetho(dt, aes(x=t, z=moving), multiplot = 2 ) + stat_bar_tile_etho() + facet_wrap( ~ uid) pl Since we kept both id and uid in the metadata, we can map them again each other. The downside of using numbers, is that they are ambiguous. Say, you decide to start with more data, or to remove some, then the numbers (uid) will not match their previous id. Conversely, if you try to analyse different datasets independently, and merge results afterwards, you will end up with duplicates in uid. Therefore, only use this trick for readability and visualisation. If you want to understand facets a bit more, have a look at this tutorial. LD annotations Basics In circadian experiments, we often like to add annotations (black and white boxes) to show Dark and Light phases. We have another layer for that: pl &lt;- ggetho(dt, aes(x=t, y=moving)) + stat_pop_etho() + stat_ld_annotations() pl Changing LD colours Sometimes you want different colours to explain, for instance, that days are “subjective”(grey). pl &lt;- ggetho(dt, aes(x=t, y=moving)) + stat_pop_etho() + stat_ld_annotations(ld_colours = c(&quot;grey&quot;, &quot;black&quot;)) pl LD in the background To put the annotation in the background, we can invert the order of the layers, set the height of the annotation to 1 (100%) and add some transparency (alpha = 0.3). We also remove the outline of the boxes: pl &lt;- ggetho(dt, aes(x=t, y=moving)) + stat_ld_annotations(height=1, alpha=0.3, outline = NA) + stat_pop_etho() pl Phase and period Sometimes you want to show annotations with different phases and periods. For instance, here, we shift the LD annotations 1h forward: pl &lt;- ggetho(dt, aes(x=t, y=moving)) + stat_ld_annotations(phase = hours(1)) + stat_pop_etho() pl One can also plot over a period different from 24h, say 20h days: pl &lt;- ggetho(dt, aes(x=t, y=moving)) + stat_ld_annotations(period = hours(20)) + stat_pop_etho() pl Regime change When, you want to indicate a change in regime, say from LD to DD. A simple way is to use multiple layers with explicit start and end points: pl &lt;- ggetho(dt, aes(x=t, y=moving)) + # the default annotation layer stat_ld_annotations() + # on top of it, a second layer that # starts at day 2 thoughout day 5, # and where L colour is grey stat_ld_annotations(x_limits = days(c(2,5)), ld_colours = c(&quot;grey&quot;, &quot;black&quot; )) + stat_pop_etho() pl Coordinate and scales Plot limits As ggetho creates regular ggplot objects, which we can extend. For instance, we can change the scales. For instance, put the y scale as a percentage between 0 and 100: pl &lt;- ggetho(dt, aes(x=t, y=moving)) + stat_pop_etho() + stat_ld_annotations() pl &lt;- pl + scale_y_continuous(limits = c(0,1), labels = scales::percent) pl We can also use the same principle to zoom in a finished plot. E.g. between day one and day two: pl + coord_cartesian(xlim=c(days(1), days(2))) Time scale units By default, ggetho decides the unit of the time axis according to the range of the data. Sometimes you want to override this behaviour to force time to be in a specific unit (here hours). Using the plot above, we can add a scale: pl + ggetho::scale_x_hours() ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which ## will replace the existing scale. R actually warns you since you are replacing the scale. This is fine (as it is precisely what we wanted)! Coordinate systems Sometimes, it makes sense to use polar coordinates to show data around the clock: pl &lt;- ggetho(dt, aes(x=t, y=moving, colour=sex), time_wrap = days(1)) + stat_ld_annotations(height=.5, alpha=.2, x_limits = c(0, days(1)), outline = NA) + stat_pop_etho(geom = &quot;polygon&quot;, fill=NA) ## Warning: Ignoring unknown parameters: se pl + coord_polar() Periodograms To draw periodogram, we can use our special fucntion ggperio. The library zeitgebr generates periodogram as behavr tables. with columns for power, period … library(zeitgebr) dt[, t := ifelse(xmv(condition) == &quot;A&quot;, t, t * 1.01)] per_dt &lt;- periodogram(moving, dt, FUN = chi_sq_periodogram, resample_rate = 1/mins(5)) per_dt ## ## ==== METADATA ==== ## ## id region_id condition sex mean_moving uid ## &lt;char&gt; &lt;int&gt; &lt;char&gt; &lt;char&gt; &lt;num&gt; &lt;int&gt; ## 1: toy_experiment|01 1 A M 138 1 ## 2: toy_experiment|02 2 B M 195 2 ## 3: toy_experiment|03 3 A F 90 3 ## 4: toy_experiment|04 4 B F 118 4 ## 5: toy_experiment|05 5 A M 123 5 ## --- ## 36: toy_experiment|36 36 B F 105 36 ## 37: toy_experiment|37 37 A M 228 37 ## 38: toy_experiment|38 38 B M 78 38 ## 39: toy_experiment|39 39 A F 59 39 ## 40: toy_experiment|40 40 B F 84 40 ## ## ====== DATA ====== ## ## id period power signif_threshold p_value ## &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; ## 1: toy_experiment|01 57600 168.16799 276.4734 0.8917869 ## 2: toy_experiment|01 57960 148.22949 277.6686 0.9928340 ## 3: toy_experiment|01 58320 115.98897 278.8632 0.9999982 ## 4: toy_experiment|01 58680 62.87222 281.2511 1.0000000 ## 5: toy_experiment|01 59040 49.50442 282.4443 1.0000000 ## --- ## 6436: toy_experiment|40 113760 202.30651 493.8790 1.0000000 ## 6437: toy_experiment|40 114120 205.99758 495.0181 1.0000000 ## 6438: toy_experiment|40 114480 226.58389 497.2958 1.0000000 ## 6439: toy_experiment|40 114840 246.68587 498.4344 1.0000000 ## 6440: toy_experiment|40 115200 250.34679 499.5728 1.0000000 An average periodogram: ggperio(per_dt, aes(period, power, colour=condition)) + stat_pop_etho() Faceted by uid: ggperio(per_dt, aes(period, power, colour=condition)) + geom_line() + facet_wrap( ~ uid) Showing peaks: per_dt &lt;-find_peaks(per_dt) ggperio(per_dt, aes(period, power, colour=condition)) + geom_line() + geom_peak(colour=&quot;blue&quot;) + facet_wrap( ~ uid) Spectrograms A “spectrogram” is a visualistion of the variation in the period spectrum over time. The function spectrogram() in the zeitgebr library can generate a spectrogram for each individual in a behavr table, and return the result as a new behavr table with columns for power, period and time. ggetho makes it possible to visualise spectrograms as a heatmap, annotate them, aggregate them by condition. Whilst periodogram represents the average periodicity of a signal over the entiere time series, sometimes, period itself changes over the course of an experiment. Let’s create a dummy dataset where the period one of the groups increases after 5 days: metadata &lt;- data.table(id=sprintf(&quot;toy_experiment|%02d&quot; , 1:10), region_id=1:10, condition=c(&quot;A&quot;,&quot;B&quot;), sex=c(&quot;M&quot;,&quot;M&quot;, &quot;F&quot;, &quot;F&quot;)) ## Warning in data.table(id = sprintf(&quot;toy_experiment|%02d&quot;, 1:10), region_id ## = 1:10, : Item 4 is of size 4 but maximum size is 10 (recycled leaving ## remainder of 2 items) dt &lt;- toy_activity_data(metadata, seed=107, duration = days(10)) # to emulate a time ticking 1.2 times slower after day 5 for group A only dt[, t := ifelse(xmv(condition) == &quot;A&quot;&amp; t &gt; days(5), 1.2 * ( t - days(5)) + days(5), t)] # the average activity over time ggetho(dt, aes(y=moving, colour=condition)) + stat_pop_etho() In this simple case, a periodogram would report poorly the overall behaviour of group A and give no information about the time of the change. On the other hand, with a spectrogram, we should clearly see when the shift of period happens. This spectrograms resulting from spectrogram() can be vusualised with our function, ggspectro(): spect_dt &lt;- spectrogram(moving, dt, period_range = c(hours(6), hours(28))) ggspectro(spect_dt) + stat_tile_etho() + scale_y_hours(name= &quot;Period&quot;, log=T) + # log axis for period facet_wrap(~ condition) + stat_ld_annotations() We see that at day 5, group A, but not group B, shifts its period from 12 h to approximatly 15 h. Note that, in this example, we facet by condition, but we could also facet by id or, instead, average all individuals in one graph. An intesresting feature of such periodograms is that the phase information is discarded. In other word, we can average the periodograms of multiple individuals (e.g. two populations) even if they are out of phase (e.g. if they are free running). This represents a clear advantage over double-plotted actograms, which are often only relevant on single individual. Perdiodograms can also be wrapped in one day, for instance to study the modulations of period of an ultradian rhythm along the day (see ?ggspectro). Take home message ggetho provides you with a new set of stats, layers and scales to represent behaviours. In particular, ggetho and ggperio can be used to preprocess behavr tables in order to use a regular ggplot2 workflow. Next steps Sleep analysis with sleepr Circadian analysis with zeitgebr "],
["sleepr.html", "Sleep analysis Aims Prerequisites Background Getting the data Data curation Population plots Summarise data per animal Day sleep☼ – Night sleep☾ Statistics Sleep architecture Merging all statistics Take home message Next steps", " Sleep analysis Aims In this practical chapter, we will use a real experiment to learn how to: Annotate a behavr table with sleep state Use ggetho to display individual and population sleep amounts Compute average sleep within a time window Perform standard statistics on average sleep Analyse sleep architecture, sleep latency,… Prerequisites You have read about behavr tables You are familiar with ggetho, our vidualisation tool You have already read the damr tutorial Ensure you have installed behavr, damr and ggetho packages: Background This tutorial focused on sleep in Drosophila. Traditionally, activity is first scored (e.g. through beam crosses/video tracking). Then any bouts of inactivity longer than five minutes count as sleep. You can easily adapt this tutorial to scoring other models/behaviours as long as you can define two discrete states (e.g. sleep vs asleep, moving vs immobile, left vs right, …). In the DAM tutorial, we have learnt how to load data from a real DAM expriment. Since we already described it in length, it makes sense to use this experiment as an example for our sleep analysis. I will assume that you have already read and understood the DAM tutorial. The last thing we did then was loading data and scoring sleep: library(sleepr) dt &lt;- load_dam(metadata, FUN = sleepr::sleep_dam_annotation) dt Getting the data Instead of going through the whole damr tutorial again, I thought I would put the resulting data table online. Importantly, for simplicity, I have just kept replicate 1. We just need to download it and load it: library(sleepr) library(ggetho) URL &lt;- &quot;https://github.com/rethomics/rethomics.github.io/raw/source/material/sleep_dam.RData&quot; load(url(URL)) summary(dt) ## behavr table with: ## 96 individuals ## 8 metavariables ## 4 variables ## 3.68736e+05 measurements ## 1 key (id) Data curation First of all, lets visualise all our sleep data. It is important to pay critical attention to this graph in order to assess if anything has gone wrong: ggetho(dt, aes(z=asleep)) + stat_ld_annotations(height = 1)+ stat_tile_etho() Dead animals Some animals may have died during the experiment, and could be wrongly scored as asleep for very long durations. sleepr has an utility function to remove data from dead animals: # we give our curated data another name so we can see the difference dt_curated &lt;- curate_dead_animals(dt) summary(dt_curated) ## behavr table with: ## 95 individuals ## 8 metavariables ## 4 variables ## 3.57436e+05 measurements ## 1 key (id) As you can see, we now have 95 individuals vs 96 in the original data. To see which animals have been removed, we could run something like: setdiff(dt[, id, meta=T], dt_curated[, id, meta=T]) ## [1] &quot;2017-07-01 08:00:00|Monitor64.txt|26&quot; Indeed, from the tile plot nothing seem to have happened in this channel. Now let us look at the data after curation: ggetho(dt_curated, aes(z=asleep)) + stat_ld_annotations(ypos = &quot;top&quot;)+ stat_tile_etho() Animals that died too early In addition, we could want to, for instance, remove animals that did not live say longer than 2 days. Of course, you need to have a good reason to exclude some animals, and that depends on your specific experiment (this is just showing you how to do it). To remove animals that did not live longer that 2 days, we use the power of behavr tables: # we make a summary table of all lifespan for each animals lifespan_dt &lt;- dt_curated[, .(lifespan = max(t)), by=id] # we filter this table for lifespan&gt;2 and we keep the id valid_ids &lt;- lifespan_dt[lifespan &gt; days(2), id] # we apply this filter dt_curated &lt;- dt_curated[id %in% valid_ids] summary(dt_curated) ## behavr table with: ## 94 individuals ## 8 metavariables ## 4 variables ## 3.54923e+05 measurements ## 1 key (id) Trimming Generally, we want to remove a point according the experimental time. For instance, lets say we would like to keep only the first 60 hours of data (i.e. 2.5 days) dt_curated &lt;- dt_curated[t %between% c(days(0), days(2.5))] summary(dt_curated) ## behavr table with: ## 94 individuals ## 8 metavariables ## 4 variables ## 3.38494e+05 measurements ## 1 key (id) Which means we are only considering data between 0 and 2.5 days. The same principle can be used to remove the beginning of an experiment. For instance, when animals are acclimatising to their new environment. Population plots Now that we have curated our data, we can start looking at the biology. First, we make a global population plot: ggetho(dt_curated, aes(y=asleep, colour=sex)) + stat_pop_etho() + stat_ld_annotations() + facet_grid(genotype ~ .) The y axis shows the proportion of time sent sleeping, averaged for each animal within a 30min (default) time window. Then, we can wrap (average) that over one day. We also polish the y axis label: ggetho(dt_curated, aes(y=asleep, colour=sex), time_wrap = hours(24)) + stat_pop_etho() + stat_ld_annotations() + facet_grid(genotype ~ .) + scale_y_continuous(name= &quot;Fraction of time sleeping&quot;,labels = scales::percent) That gives us a good understanding of what happens at the population level. Summarise data per animal Most likely, we want to summarise sleep amount so that we have one number per animal. For instance, we can compute the overall average proportion of time spent sleeping: summary_dt &lt;- rejoin(dt_curated[, .( # this is where the computation happens sleep_fraction = mean(asleep) ), by=id]) summary_dt ## id file_info region_id ## 1: 2017-07-01 08:00:00|Monitor11.txt|01 &lt;list&gt; 1 ## 2: 2017-07-01 08:00:00|Monitor11.txt|02 &lt;list&gt; 2 ## 3: 2017-07-01 08:00:00|Monitor11.txt|03 &lt;list&gt; 3 ## 4: 2017-07-01 08:00:00|Monitor11.txt|04 &lt;list&gt; 4 ## 5: 2017-07-01 08:00:00|Monitor11.txt|05 &lt;list&gt; 5 ## --- ## 90: 2017-07-01 08:00:00|Monitor64.txt|27 &lt;list&gt; 27 ## 91: 2017-07-01 08:00:00|Monitor64.txt|29 &lt;list&gt; 29 ## 92: 2017-07-01 08:00:00|Monitor64.txt|30 &lt;list&gt; 30 ## 93: 2017-07-01 08:00:00|Monitor64.txt|31 &lt;list&gt; 31 ## 94: 2017-07-01 08:00:00|Monitor64.txt|32 &lt;list&gt; 32 ## experiment_id start_datetime stop_datetime ## 1: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 2: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 3: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 4: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 5: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## --- ## 90: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 91: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 92: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 93: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 94: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## sex genotype replicate sleep_fraction ## 1: M A 1 0.7600666 ## 2: M A 1 0.7550680 ## 3: M A 1 0.8214385 ## 4: M A 1 0.7528464 ## 5: M A 1 0.8044988 ## --- ## 90: F C 1 0.7661761 ## 91: F C 1 0.6725909 ## 92: F C 1 0.7622883 ## 93: F C 1 0.7967231 ## 94: F C 1 0.9208553 With rejoin, we have put our summary and metadata together, which is suitable for standars graphics/statictics. For instance, if we are interested in the effect of sleep and genotype on sleep amount, we can make a faceted boxplot, and also add individual points to show all data. ggplot(summary_dt, aes(x=sex, y=sleep_fraction, fill=sex)) + geom_boxplot(outlier.colour = NA) + geom_jitter(alpha=.5) + facet_grid( genotype ~ .) + scale_y_continuous(name= &quot;Fraction of time sleeping&quot;,labels = scales::percent) Day sleep☼ – Night sleep☾ Often, we want to compare amount of sleep during the day vs night as they are different processes. Adding some phase information The simplest way to do that is to start by adding some phase information to our data. L phase (light) should be any point between ZT0 and ZT12 – [0,12), [24,36), … D phase (dark) should be any point between ZT12 and ZT24 – [12,24), [36,48), … Numerically, this can be done very simply using a modulo operation on time. In R, modulo is %%. The following line creates a new variable in dt. This variable is: &quot;L&quot; when the remainder of the division of (the corresponding) t by 24h is lower than 12h &quot;D&quot; otherwise dt_curated[, phase := ifelse(t %% hours(24) &lt; hours(12), &quot;L&quot;, &quot;D&quot;)] Since we have this column, we can make an improved summary (pay special attention to the last columns): summary_dt &lt;- rejoin(dt_curated[, .( # this is where the computation happens sleep_fraction_all = mean(asleep), sleep_fraction_l = mean(asleep[phase == &quot;L&quot;]), sleep_fraction_d = mean(asleep[phase == &quot;D&quot;]) ), ,by=id]) summary_dt ## id file_info region_id ## 1: 2017-07-01 08:00:00|Monitor11.txt|01 &lt;list&gt; 1 ## 2: 2017-07-01 08:00:00|Monitor11.txt|02 &lt;list&gt; 2 ## 3: 2017-07-01 08:00:00|Monitor11.txt|03 &lt;list&gt; 3 ## 4: 2017-07-01 08:00:00|Monitor11.txt|04 &lt;list&gt; 4 ## 5: 2017-07-01 08:00:00|Monitor11.txt|05 &lt;list&gt; 5 ## --- ## 90: 2017-07-01 08:00:00|Monitor64.txt|27 &lt;list&gt; 27 ## 91: 2017-07-01 08:00:00|Monitor64.txt|29 &lt;list&gt; 29 ## 92: 2017-07-01 08:00:00|Monitor64.txt|30 &lt;list&gt; 30 ## 93: 2017-07-01 08:00:00|Monitor64.txt|31 &lt;list&gt; 31 ## 94: 2017-07-01 08:00:00|Monitor64.txt|32 &lt;list&gt; 32 ## experiment_id start_datetime stop_datetime ## 1: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 2: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 3: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 4: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 5: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## --- ## 90: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 91: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 92: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 93: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 94: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## sex genotype replicate sleep_fraction_all sleep_fraction_l ## 1: M A 1 0.7600666 0.8500000 ## 2: M A 1 0.7550680 0.7861111 ## 3: M A 1 0.8214385 0.8356481 ## 4: M A 1 0.7528464 0.8555556 ## 5: M A 1 0.8044988 0.8444444 ## --- ## 90: F C 1 0.7661761 0.7504630 ## 91: F C 1 0.6725909 0.6370370 ## 92: F C 1 0.7622883 0.7523148 ## 93: F C 1 0.7967231 0.7773148 ## 94: F C 1 0.9208553 0.9643519 ## sleep_fraction_d ## 1: 0.6252602 ## 2: 0.7085357 ## 3: 0.8001388 ## 4: 0.5988897 ## 5: 0.7446218 ## --- ## 90: 0.7897294 ## 91: 0.7258848 ## 92: 0.7772380 ## 93: 0.8258154 ## 94: 0.8556558 Now, we have three new variables: sleep_fraction_all, sleep_fraction_l and sleep_fraction_d. We can just replace the y axis with our variable of interest (e.g. sleep in D phase): ggplot(summary_dt, aes(x=sex, y=sleep_fraction_d, fill=sex)) + geom_boxplot(outlier.colour = NA) + geom_jitter(alpha=.5) + facet_grid(genotype ~ .) + scale_y_continuous(name= &quot;Fraction of time sleeping&quot;,labels = scales::percent) If we wanted a plot with all three values, we could “melt” our data, picking all columns starting with &quot;sleep_fraction_&quot; as “measurment variables”: summary_dt_melted &lt;- melt(summary_dt, measure.vars = patterns(&quot;sleep_fraction_&quot;), variable.name = &quot;phase&quot;, value.name = &quot;sleep_fraction&quot;) Now, instead of three columns for the three variables, we have two columns, one for the actual value and one to describe the phase (all vs L vs D). This makes it convenient to use with ggplot: ggplot(summary_dt_melted, aes(x=phase, y=sleep_fraction, fill=sex)) + geom_boxplot(outlier.colour = NA) + geom_jitter(alpha=.5) + facet_grid(genotype ~ .) + scale_y_continuous(name= &quot;Fraction of time sleeping&quot;,labels = scales::percent) Statistics Often, you want to go further than representing the data, and compute statistics. R was designed primarilly as statistical progamming language. As a result, a tremendous variety of simple and elaborate statics are implemented. This section will not go in the details of what you can do in terms of stats, many authors have already published fantastic resources on this subject. Instead, we present very simple examples of what can be done. At this stage, what you do depends very much on your question, your knowledge of statistics and how much effort you want to invest. Pairwise Wilcoxon tests Say we wanted to compute, for females only, all pairwise tests between all genotype groups (A vs B, B vs C and C vs A). This could be formulated as: pairwise.wilcox.test(summary_dt[sex==&quot;F&quot;, sleep_fraction_all], summary_dt[sex==&quot;F&quot;, genotype]) ## Warning in wilcox.test.default(xi, xj, paired = paired, ...): cannot ## compute exact p-value with ties ## Warning in wilcox.test.default(xi, xj, paired = paired, ...): cannot ## compute exact p-value with ties ## ## Pairwise comparisons using Wilcoxon rank sum test ## ## data: summary_dt[sex == &quot;F&quot;, sleep_fraction_all] and summary_dt[sex == &quot;F&quot;, genotype] ## ## A B ## B 0.0047 - ## C 4.1e-08 0.0018 ## ## P value adjustment method: holm We get a matrix showing us all p-values. You could do that also within males, as long as we replace sex == &quot;F&quot; by sex == &quot;M&quot; Two way anova If we are interested in the effect of sex AND genotype, as well as their interaction, we can model our response variable with a formula: sleep_fraction_all ~ sex * genotype: model &lt;- aov(sleep_fraction_all ~ sex * genotype, summary_dt) summary(model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## sex 1 0.3235 0.3235 26.39 1.66e-06 *** ## genotype 2 0.4417 0.2209 18.02 2.76e-07 *** ## sex:genotype 2 0.3874 0.1937 15.80 1.37e-06 *** ## Residuals 88 1.0786 0.0123 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This shows a strong affect of sex, genotype and their interaction on sleep amount. There are several way to follow up. For instance see this short tutorial. Sleep architecture Proportion alone is not always a sufficent measure to fully descibe the dynamics of sleep. One way to go further is to study sleep as a series of bouts. Bout analysis The function bout_analysis(), in sleepr is designed for that. We would use it like that: bout_dt &lt;- bout_analysis(asleep, dt_curated) The result is a new behavr table, with a few differences compared to the ones we used before: Each row in the data describes a bout The bout can take the values asleep=TRUE or asleep=FALSE (sleep bout or wake bout, respectively) t is the onset of the bout (in seconds) duration is length of the bout (in seconds) Note that you can use this function to study bouts of other discrete behaviours. For now, we are only interested in sleep bout, so we filter for asleep == TRUE. We also remove the, now redundant, asleep column: bout_dt &lt;- bout_dt[asleep == TRUE, -&quot;asleep&quot;] Bout length vs time of the day We can use ggetho to show how the average bout length depends on the time of the onset of the bout. ggetho(bout_dt, aes(y=duration / 60, colour=sex), time_wrap = hours(24)) + stat_pop_etho() + facet_grid(genotype ~ .) + scale_y_continuous(name= &quot;Bout length (min)&quot;) Note that this is a bit noisy as we only have a few animals per combination of treatment. Architecture description One can count the total number of bouts and average bout duration for each individual like so: bout_dt[, .(n_bouts = .N, mean_bout_length = mean(duration)), by=id] ## ## ==== METADATA ==== ## ## id file_info region_id ## &lt;fctr&gt; &lt;list&gt; &lt;int&gt; ## 1: 2017-07-01 08:00:00|Monitor11.txt|01 &lt;list&gt; 1 ## 2: 2017-07-01 08:00:00|Monitor11.txt|02 &lt;list&gt; 2 ## 3: 2017-07-01 08:00:00|Monitor11.txt|03 &lt;list&gt; 3 ## 4: 2017-07-01 08:00:00|Monitor11.txt|04 &lt;list&gt; 4 ## 5: 2017-07-01 08:00:00|Monitor11.txt|05 &lt;list&gt; 5 ## --- ## 90: 2017-07-01 08:00:00|Monitor64.txt|27 &lt;list&gt; 27 ## 91: 2017-07-01 08:00:00|Monitor64.txt|29 &lt;list&gt; 29 ## 92: 2017-07-01 08:00:00|Monitor64.txt|30 &lt;list&gt; 30 ## 93: 2017-07-01 08:00:00|Monitor64.txt|31 &lt;list&gt; 31 ## 94: 2017-07-01 08:00:00|Monitor64.txt|32 &lt;list&gt; 32 ## experiment_id start_datetime stop_datetime ## &lt;char&gt; &lt;POSc&gt; &lt;char&gt; ## 1: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 2: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 3: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 4: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 5: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## --- ## 90: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 91: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 92: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 93: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 94: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## sex genotype replicate ## &lt;char&gt; &lt;char&gt; &lt;int&gt; ## 1: M A 1 ## 2: M A 1 ## 3: M A 1 ## 4: M A 1 ## 5: M A 1 ## --- ## 90: F C 1 ## 91: F C 1 ## 92: F C 1 ## 93: F C 1 ## 94: F C 1 ## ## ====== DATA ====== ## ## id n_bouts mean_bout_length ## &lt;fctr&gt; &lt;int&gt; &lt;num&gt; ## 1: 2017-07-01 08:00:00|Monitor11.txt|01 80 2052.000 ## 2: 2017-07-01 08:00:00|Monitor11.txt|02 109 1496.147 ## 3: 2017-07-01 08:00:00|Monitor11.txt|03 97 1829.072 ## 4: 2017-07-01 08:00:00|Monitor11.txt|04 95 1711.579 ## 5: 2017-07-01 08:00:00|Monitor11.txt|05 59 2945.085 ## --- ## 90: 2017-07-01 08:00:00|Monitor64.txt|27 75 2206.400 ## 91: 2017-07-01 08:00:00|Monitor64.txt|29 142 1023.380 ## 92: 2017-07-01 08:00:00|Monitor64.txt|30 99 1663.636 ## 93: 2017-07-01 08:00:00|Monitor64.txt|31 67 2568.358 ## 94: 2017-07-01 08:00:00|Monitor64.txt|32 36 5525.000 You could apply the approach presented before to compute statistics according the the phase (night vs day bouts). Latency to sleep The latency describes how long it takes for an animal to initiate its first sleep bout. Some researchers are also interested in the latency to the longest bout. In this example, lets say we focus on the second day (and not the night – 24 to 36 hours). bout_dt_second_day &lt;- bout_dt[t %between% c(days(1), days(1) + hours(12))] # We express t relatively to the first day bout_dt_second_day[, t:= t - days(1)] bout_summary &lt;- bout_dt_second_day[,.( latency = t[1], # the first bout is at t[1] first_bout_length = duration[1], latency_to_longest_bout = t[which.max(duration)], length_longest_bout = max(duration), n_bouts = .N, mean_bout_length = mean(duration) ), by=id] bout_summary ## ## ==== METADATA ==== ## ## id file_info region_id ## &lt;fctr&gt; &lt;list&gt; &lt;int&gt; ## 1: 2017-07-01 08:00:00|Monitor11.txt|01 &lt;list&gt; 1 ## 2: 2017-07-01 08:00:00|Monitor11.txt|02 &lt;list&gt; 2 ## 3: 2017-07-01 08:00:00|Monitor11.txt|03 &lt;list&gt; 3 ## 4: 2017-07-01 08:00:00|Monitor11.txt|04 &lt;list&gt; 4 ## 5: 2017-07-01 08:00:00|Monitor11.txt|05 &lt;list&gt; 5 ## --- ## 89: 2017-07-01 08:00:00|Monitor64.txt|27 &lt;list&gt; 27 ## 90: 2017-07-01 08:00:00|Monitor64.txt|29 &lt;list&gt; 29 ## 91: 2017-07-01 08:00:00|Monitor64.txt|30 &lt;list&gt; 30 ## 92: 2017-07-01 08:00:00|Monitor64.txt|31 &lt;list&gt; 31 ## 93: 2017-07-01 08:00:00|Monitor64.txt|32 &lt;list&gt; 32 ## experiment_id start_datetime stop_datetime ## &lt;char&gt; &lt;POSc&gt; &lt;char&gt; ## 1: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 2: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 3: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 4: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 5: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## --- ## 89: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 90: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 91: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 92: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 93: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## sex genotype replicate ## &lt;char&gt; &lt;char&gt; &lt;int&gt; ## 1: M A 1 ## 2: M A 1 ## 3: M A 1 ## 4: M A 1 ## 5: M A 1 ## --- ## 89: F C 1 ## 90: F C 1 ## 91: F C 1 ## 92: F C 1 ## 93: F C 1 ## ## ====== DATA ====== ## ## id latency first_bout_length ## &lt;fctr&gt; &lt;num&gt; &lt;num&gt; ## 1: 2017-07-01 08:00:00|Monitor11.txt|01 2940 360 ## 2: 2017-07-01 08:00:00|Monitor11.txt|02 3540 420 ## 3: 2017-07-01 08:00:00|Monitor11.txt|03 4500 300 ## 4: 2017-07-01 08:00:00|Monitor11.txt|04 1080 720 ## 5: 2017-07-01 08:00:00|Monitor11.txt|05 2760 300 ## --- ## 89: 2017-07-01 08:00:00|Monitor64.txt|27 6120 300 ## 90: 2017-07-01 08:00:00|Monitor64.txt|29 900 540 ## 91: 2017-07-01 08:00:00|Monitor64.txt|30 2100 300 ## 92: 2017-07-01 08:00:00|Monitor64.txt|31 4380 420 ## 93: 2017-07-01 08:00:00|Monitor64.txt|32 2160 5640 ## latency_to_longest_bout length_longest_bout n_bouts mean_bout_length ## &lt;num&gt; &lt;num&gt; &lt;int&gt; &lt;num&gt; ## 1: 27000 3300 25 1392.0000 ## 2: 17760 6300 23 1468.6957 ## 3: 37920 3600 29 1204.1379 ## 4: 30600 6240 28 1350.0000 ## 5: 14820 5880 15 2540.0000 ## --- ## 89: 14160 6720 19 1800.0000 ## 90: 11400 1980 42 625.7143 ## 91: 35820 4560 25 1329.6000 ## 92: 21360 14760 19 1664.2105 ## 93: 26820 15360 6 6690.0000 For good measure, I also added number of bouts and average bout length as we have seen before. You can, of course, use these results to plot things like the relationship between bout length and bout number: ggplot(rejoin(bout_summary), aes(n_bouts, mean_bout_length, colour=sex)) + geom_point() + facet_grid(genotype ~ .) + scale_x_continuous(name=&quot;Number of bouts&quot;) + scale_y_continuous(name=&quot;Average bout duration (s)&quot;) Always be critical about what you do. For instance, what whould be the latency to sleep of an animal that, in the period of observation, does not sleep? Merging all statistics Earlier, we made a summary_dt in which we computed some statitics such as sleep fraction in L and D phase. In addition, we have now a bout_summary where we have other variables. These data have both one row per animal. Ideally, we could “merge” them into a single table that has all the individual statistics. This way we can study the relationship, say, between sleep amount and latency. In order to do that, we perform a so called “join”: overall_summary &lt;- summary_dt[bout_summary] overall_summary ## id file_info region_id ## 1: 2017-07-01 08:00:00|Monitor11.txt|01 &lt;list&gt; 1 ## 2: 2017-07-01 08:00:00|Monitor11.txt|02 &lt;list&gt; 2 ## 3: 2017-07-01 08:00:00|Monitor11.txt|03 &lt;list&gt; 3 ## 4: 2017-07-01 08:00:00|Monitor11.txt|04 &lt;list&gt; 4 ## 5: 2017-07-01 08:00:00|Monitor11.txt|05 &lt;list&gt; 5 ## --- ## 89: 2017-07-01 08:00:00|Monitor64.txt|27 &lt;list&gt; 27 ## 90: 2017-07-01 08:00:00|Monitor64.txt|29 &lt;list&gt; 29 ## 91: 2017-07-01 08:00:00|Monitor64.txt|30 &lt;list&gt; 30 ## 92: 2017-07-01 08:00:00|Monitor64.txt|31 &lt;list&gt; 31 ## 93: 2017-07-01 08:00:00|Monitor64.txt|32 &lt;list&gt; 32 ## experiment_id start_datetime stop_datetime ## 1: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 2: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 3: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 4: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## 5: 2017-07-01 08:00:00|Monitor11.txt 2017-07-01 08:00:00 2017-07-04 ## --- ## 89: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 90: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 91: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 92: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## 93: 2017-07-01 08:00:00|Monitor64.txt 2017-07-01 08:00:00 2017-07-04 ## sex genotype replicate sleep_fraction_all sleep_fraction_l ## 1: M A 1 0.7600666 0.8500000 ## 2: M A 1 0.7550680 0.7861111 ## 3: M A 1 0.8214385 0.8356481 ## 4: M A 1 0.7528464 0.8555556 ## 5: M A 1 0.8044988 0.8444444 ## --- ## 89: F C 1 0.7661761 0.7504630 ## 90: F C 1 0.6725909 0.6370370 ## 91: F C 1 0.7622883 0.7523148 ## 92: F C 1 0.7967231 0.7773148 ## 93: F C 1 0.9208553 0.9643519 ## sleep_fraction_d latency first_bout_length latency_to_longest_bout ## 1: 0.6252602 2940 360 27000 ## 2: 0.7085357 3540 420 17760 ## 3: 0.8001388 4500 300 37920 ## 4: 0.5988897 1080 720 30600 ## 5: 0.7446218 2760 300 14820 ## --- ## 89: 0.7897294 6120 300 14160 ## 90: 0.7258848 900 540 11400 ## 91: 0.7772380 2100 300 35820 ## 92: 0.8258154 4380 420 21360 ## 93: 0.8556558 2160 5640 26820 ## length_longest_bout n_bouts mean_bout_length ## 1: 3300 25 1392.0000 ## 2: 6300 23 1468.6957 ## 3: 3600 29 1204.1379 ## 4: 6240 28 1350.0000 ## 5: 5880 15 2540.0000 ## --- ## 89: 6720 19 1800.0000 ## 90: 1980 42 625.7143 ## 91: 4560 25 1329.6000 ## 92: 14760 19 1664.2105 ## 93: 15360 6 6690.0000 ggplot(overall_summary, aes(latency / 60, sleep_fraction_l, colour=sex)) + geom_point() + geom_smooth(method=&quot;lm&quot;, alpha=.1)+ facet_grid(genotype ~ .) Take home message Data analysis and visualisation is about translating your biological questions to another language. Problems in emerging areas of science can be very rich so they should be matched with the equally rich grammar that only a programming language can provide. This tutorial was very simple and does not pretend to provied a canonical sleep analysis. Instead, see it as a set of building blocks that you can use and rearrange to address your own questions. Next steps Visualise data with ggetho Circadian analysis with zeitgebr "],
["zeitgebr.html", "Circadian rhythm analysis Aims Prerequisites Background Getting the data Quality control Double plotted actograms Periodograms Next steps", " Circadian rhythm analysis Aims In this practical chapter, we will use a real experiment to learn how to: Use zeitgebr to compute periodograms Use ggetho to draw double plotted actograms Average periodograms vs conditions Find and compare peaks Prerequisites You have read about behavr tables You are familiar with ggetho, our visualisation tool You have already read the damr tutorial Ensure you have installed behavr, damr, ggetho and zeitgebr packages Background This tutorial focuses on circadian rhythm in Drosophila, but can be adapted easily to investigate periodicity in different contexts. In it, we will use a DAM dataset that is provided (and preloaded) within the damr package. Getting the data Getting the data is a lot simpler than in the other tutorials as it is already on your computer: library(damr) library(zeitgebr) library(ggetho) # We load the data data(dams_sample) # We copy the data to our own variable so we can alter it dt &lt;- copy(dams_sample) This data set is a recording of 32 animals, in DD. Let us print the metadata to understand the experiment a little bit: summary(dt) ## behavr table with: ## 32 individuals ## 6 metavariables ## 2 variables ## 4.1504e+05 measurements ## 1 key (id) print(dt[meta=T]) ## id file_info region_id ## 1: 2017-01-16 08:00:00|dams_sample.txt|01 &lt;list&gt; 1 ## 2: 2017-01-16 08:00:00|dams_sample.txt|02 &lt;list&gt; 2 ## 3: 2017-01-16 08:00:00|dams_sample.txt|03 &lt;list&gt; 3 ## 4: 2017-01-16 08:00:00|dams_sample.txt|04 &lt;list&gt; 4 ## 5: 2017-01-16 08:00:00|dams_sample.txt|05 &lt;list&gt; 5 ## --- ## 28: 2017-01-16 08:00:00|dams_sample.txt|28 &lt;list&gt; 28 ## 29: 2017-01-16 08:00:00|dams_sample.txt|29 &lt;list&gt; 29 ## 30: 2017-01-16 08:00:00|dams_sample.txt|30 &lt;list&gt; 30 ## 31: 2017-01-16 08:00:00|dams_sample.txt|31 &lt;list&gt; 31 ## 32: 2017-01-16 08:00:00|dams_sample.txt|32 &lt;list&gt; 32 ## experiment_id start_datetime ## 1: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 2: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 3: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 4: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 5: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## --- ## 28: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 29: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 30: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 31: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 32: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## stop_datetime period_group ## 1: 2017-01-26 00:09:00 long ## 2: 2017-01-26 00:09:00 long ## 3: 2017-01-26 00:09:00 long ## 4: 2017-01-26 00:09:00 long ## 5: 2017-01-26 00:09:00 long ## --- ## 28: 2017-01-26 00:09:00 wt ## 29: 2017-01-26 00:09:00 wt ## 30: 2017-01-26 00:09:00 wt ## 31: 2017-01-26 00:09:00 wt ## 32: 2017-01-26 00:09:00 wt We can confirm we have 32 individuals. They are described by one of three “period groups” (e.g. a genotype or treatment). This is encoded in period_group metavariable can be either a “short”“,”long“” or wild-type (“wt”) period. Quality control This data is fairly clean already, so we will not do much. For most purposes, you can apply the same principles as data curation for sleep analysis. Regime changes In general (but not here), you will have a period of LD “baseline” preceding a change to a DD regime. I suggest to encode that in the following way: Manually add a column baseline_days in your metadata that defines the number of days of baseline. Sustract the number of baseline days to all time points: dt[, t := t - days(xmv(baseline_days))] As a result, t = 0 now means “ZT0 of the transition day”. This makes a lot of sense as any baseline point is at a negative time (t &lt; 0), whilst t &gt; 0 is for DD. The nice thing is that you can work with data (e.g. replicates) that have different baseline duration as now all time points are relative to the regime change. Data enrichment By defaut, DAM data only has variables t and activity. The latter being the number of beam crosses over a time bin (e.g. one minute). We could define a variable moving that is TRUE when and only when activity &gt; 0, and FALSE otherwise: dt[, moving := activity &gt; 0] Overview plots The first representation we show could be the activity of all animals over time in the same page. This would help to spot outliers. We simply do that with our tile plot. Note how we have changed the LD colours to grey and black, grey being for subjective days (remember, we are in DD). ggetho(dt, aes(z=activity)) + stat_ld_annotations(ld_colours = c(&quot;grey&quot;, &quot;black&quot;))+ stat_tile_etho() Note that you can substitute other variables you have in dt (e.g. moving) for activity. We see that two animals (region 5 and 10) seem to have died/escaped before the end of the experiment. There is no right thing to do regarding dead animals. You could keep the data before death, or remove them altogether. It is eventually your responsibility to decide what to do with dead animals. Here, I simply remove data after death: library(sleepr) dt_curated &lt;- curate_dead_animals(dt) summary(dt_curated) ## behavr table with: ## 32 individuals ## 6 metavariables ## 3 variables ## 4.06864e+05 measurements ## 1 key (id) ggetho(dt_curated, aes(z=activity)) + stat_ld_annotations(ld_colours = c(&quot;grey&quot;, &quot;black&quot;))+ stat_tile_etho() Our curated data is now stored in dt_curated. Double plotted actograms At the moment, the id is a very long string (e.g. &quot;2017-01-16 08:00:00|dams_sample.txt|01&quot;). It has the advantage to be unambiguous, but it is difficult to plot. To help plotting, we can make a new variable that is simply a different number for each individual. Lets call it uid: dt_curated[, uid := 1 : .N, meta=T] # We can map uid to id dt_curated[, .(id, uid) ,meta=T] ## id uid ## 1: 2017-01-16 08:00:00|dams_sample.txt|01 1 ## 2: 2017-01-16 08:00:00|dams_sample.txt|02 2 ## 3: 2017-01-16 08:00:00|dams_sample.txt|03 3 ## 4: 2017-01-16 08:00:00|dams_sample.txt|04 4 ## 5: 2017-01-16 08:00:00|dams_sample.txt|05 5 ## --- ## 28: 2017-01-16 08:00:00|dams_sample.txt|28 28 ## 29: 2017-01-16 08:00:00|dams_sample.txt|29 29 ## 30: 2017-01-16 08:00:00|dams_sample.txt|30 30 ## 31: 2017-01-16 08:00:00|dams_sample.txt|31 31 ## 32: 2017-01-16 08:00:00|dams_sample.txt|32 32 As you see, we do keep id as a reference but uid for convenience in graphs. To make a double plotted actogram, we use ggetho. Read more about that in the visualisation tutorial. Briefly, we set multiplot to 2 (3 would be a triple plotted) one. The variable of interest is on the z axis. Note that instead of moving, you could plot raw activity. Then, we use bar height to show the amount of movement (we could use stat_tile_etho which shows the variable of interest as a colour with pixel intensity). Lastly, we split the graph by uid, so that each facet is a single animal. ggetho(dt_curated, aes(z = moving), multiplot = 2) + stat_bar_tile_etho() + facet_wrap( ~ uid, ncol = 8) Interestingly, you can use formula in facet to show or sort with other metavariables: ggetho(dt_curated, aes(z=moving), multiplot = 2) + stat_bar_tile_etho() + facet_wrap( ~ period_group + uid, ncol=8, labeller = label_wrap_gen(multi_line=FALSE)) Now, we know which genotype matches each uid. In your own work, you could generalise this concept to display more metavariables. For instance, if you had a sex metavariable you could do: facet_wrap( ~ period_group + sex + uid, ...). Periodograms Computation An important part of circadian research is to compute the periodicity of the free running clock of multiple individuals. Mathematically, we would like to build a preiodogram. That is, generally speaking, a representation of the density (i.e. power) of a signal at different periods (or frequencies). In addition, a periodogram associates to each pair of power-period a significance level. There are many algorithms to compute periodograms. In zeitgebr, we have so far implemented: ac_periodogram – An autocorrelation based method ls_periodogram – Lomb-Scargle algorithm chi_sq_periodogram – A \\(\\chi{}^2\\) based one See ?periodogram_methods for references. In order to compute periodograms, we use the periodogram() function. We need to define which variable we want to study (e.g. moving or activity). Then, we provide our data. The methods described above can be passed as an argument. For instance, if we want to analyse, with the \\(\\chi{}^2\\) method, activity: per_xsq_dt &lt;- periodogram(activity, dt_curated, FUN = chi_sq_periodogram) per_xsq_dt ## ## ==== METADATA ==== ## ## id file_info region_id ## &lt;fctr&gt; &lt;list&gt; &lt;int&gt; ## 1: 2017-01-16 08:00:00|dams_sample.txt|01 &lt;list&gt; 1 ## 2: 2017-01-16 08:00:00|dams_sample.txt|02 &lt;list&gt; 2 ## 3: 2017-01-16 08:00:00|dams_sample.txt|03 &lt;list&gt; 3 ## 4: 2017-01-16 08:00:00|dams_sample.txt|04 &lt;list&gt; 4 ## 5: 2017-01-16 08:00:00|dams_sample.txt|05 &lt;list&gt; 5 ## --- ## 28: 2017-01-16 08:00:00|dams_sample.txt|28 &lt;list&gt; 28 ## 29: 2017-01-16 08:00:00|dams_sample.txt|29 &lt;list&gt; 29 ## 30: 2017-01-16 08:00:00|dams_sample.txt|30 &lt;list&gt; 30 ## 31: 2017-01-16 08:00:00|dams_sample.txt|31 &lt;list&gt; 31 ## 32: 2017-01-16 08:00:00|dams_sample.txt|32 &lt;list&gt; 32 ## experiment_id start_datetime ## &lt;char&gt; &lt;POSc&gt; ## 1: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 2: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 3: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 4: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 5: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## --- ## 28: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 29: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 30: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 31: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 32: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## stop_datetime period_group uid ## &lt;fctr&gt; &lt;fctr&gt; &lt;int&gt; ## 1: 2017-01-26 00:09:00 long 1 ## 2: 2017-01-26 00:09:00 long 2 ## 3: 2017-01-26 00:09:00 long 3 ## 4: 2017-01-26 00:09:00 long 4 ## 5: 2017-01-26 00:09:00 long 5 ## --- ## 28: 2017-01-26 00:09:00 wt 28 ## 29: 2017-01-26 00:09:00 wt 29 ## 30: 2017-01-26 00:09:00 wt 30 ## 31: 2017-01-26 00:09:00 wt 31 ## 32: 2017-01-26 00:09:00 wt 32 ## ## ====== DATA ====== ## ## id period power ## &lt;fctr&gt; &lt;num&gt; &lt;num&gt; ## 1: 2017-01-16 08:00:00|dams_sample.txt|01 57600 34.17475 ## 2: 2017-01-16 08:00:00|dams_sample.txt|01 57960 34.17475 ## 3: 2017-01-16 08:00:00|dams_sample.txt|01 58320 59.19905 ## 4: 2017-01-16 08:00:00|dams_sample.txt|01 58680 59.19905 ## 5: 2017-01-16 08:00:00|dams_sample.txt|01 59040 37.79175 ## --- ## 5148: 2017-01-16 08:00:00|dams_sample.txt|32 113760 100.21510 ## 5149: 2017-01-16 08:00:00|dams_sample.txt|32 114120 124.12366 ## 5150: 2017-01-16 08:00:00|dams_sample.txt|32 114480 124.12366 ## 5151: 2017-01-16 08:00:00|dams_sample.txt|32 114840 127.12975 ## 5152: 2017-01-16 08:00:00|dams_sample.txt|32 115200 127.12975 ## signif_threshold p_value ## &lt;num&gt; &lt;num&gt; ## 1: 116.7888 0.9991894 ## 2: 116.7888 0.9991894 ## 3: 118.1251 0.6793185 ## 4: 118.1251 0.6793185 ## 5: 119.4588 0.9979513 ## --- ## 5148: 196.2269 0.9561612 ## 5149: 197.4675 0.5556435 ## 5150: 197.4675 0.5556435 ## 5151: 198.7071 0.5051096 ## 5152: 198.7071 0.5051096 The result is another behavr table, with the same metadata. The data however, is now a list of power vs period (and significance). Have a look at the other options (see ?periodogram). Peaks finding Often, we want to know which are the peak periods in a periodogram. This can be achieved thanks to the find_peaks function. By default, it finds a maximum of three peaks, which are sorted by their power (relative to the significance thershold). Peaks that are not signifant are not accounted for (see the alpha argument). per_xsq_dt &lt;- find_peaks(per_xsq_dt) per_xsq_dt This function annotates our data by adding a column named &quot;peak&quot;. Whenever the row corresponds to a peak, it puts a number and NA otherwise. The number is the rank of the peak (1 being the first/tallest one). Visualisation In ggetho, the function ggperio() is designed specifically for displaying periodograms. One could plot all the periodograms like so: ggperio(per_xsq_dt) + geom_line(aes(group = id, colour=period_group)) But it is very hard to read, so we will facet per uid. In addition, we can use a special geometry (geom_peak) to show the values of the peak. We draw a signifiance line as well, just using the signif_threshold variable: ggperio(per_xsq_dt) + geom_line(aes(group = id, colour = period_group)) + geom_peak(col = &quot;black&quot;) + geom_line(aes(y = signif_threshold)) + facet_wrap(~ uid, ncol = 8) Instead of drawing only the first peak, you could draw, for instance, the first and second (geom_peak(peak_rank = 1:2)) or, only the second (geom_peak(peak_rank = 2)). An interesting thing to do is a population average periodogram. In this graph, the solid lines are the average power per group, whilst the shaded areas are ± standard error: ggperio(per_xsq_dt, aes( y = power - signif_threshold, colour=period_group)) + stat_pop_etho() Exctract the peak values At some point, you would like to summarise each animal by, say, it first peak. Then, you can look at whether there are significant differences in peak periodicity vs genotype. Doing that is quite straightforward. First, we select only rows where peak is exactly 1, then rejoin our table to its metadata: summary_dt &lt;- rejoin(per_xsq_dt[peak==1]) summary_dt ## id file_info region_id ## 1: 2017-01-16 08:00:00|dams_sample.txt|01 &lt;list&gt; 1 ## 2: 2017-01-16 08:00:00|dams_sample.txt|02 &lt;list&gt; 2 ## 3: 2017-01-16 08:00:00|dams_sample.txt|03 &lt;list&gt; 3 ## 4: 2017-01-16 08:00:00|dams_sample.txt|04 &lt;list&gt; 4 ## 5: 2017-01-16 08:00:00|dams_sample.txt|05 &lt;list&gt; 5 ## 6: 2017-01-16 08:00:00|dams_sample.txt|06 &lt;list&gt; 6 ## 7: 2017-01-16 08:00:00|dams_sample.txt|07 &lt;list&gt; 7 ## 8: 2017-01-16 08:00:00|dams_sample.txt|08 &lt;list&gt; 8 ## 9: 2017-01-16 08:00:00|dams_sample.txt|09 &lt;list&gt; 9 ## 10: 2017-01-16 08:00:00|dams_sample.txt|10 &lt;list&gt; 10 ## 11: 2017-01-16 08:00:00|dams_sample.txt|11 &lt;list&gt; 11 ## 12: 2017-01-16 08:00:00|dams_sample.txt|12 &lt;list&gt; 12 ## 13: 2017-01-16 08:00:00|dams_sample.txt|13 &lt;list&gt; 13 ## 14: 2017-01-16 08:00:00|dams_sample.txt|14 &lt;list&gt; 14 ## 15: 2017-01-16 08:00:00|dams_sample.txt|16 &lt;list&gt; 16 ## 16: 2017-01-16 08:00:00|dams_sample.txt|17 &lt;list&gt; 17 ## 17: 2017-01-16 08:00:00|dams_sample.txt|20 &lt;list&gt; 20 ## 18: 2017-01-16 08:00:00|dams_sample.txt|21 &lt;list&gt; 21 ## 19: 2017-01-16 08:00:00|dams_sample.txt|22 &lt;list&gt; 22 ## 20: 2017-01-16 08:00:00|dams_sample.txt|23 &lt;list&gt; 23 ## 21: 2017-01-16 08:00:00|dams_sample.txt|24 &lt;list&gt; 24 ## 22: 2017-01-16 08:00:00|dams_sample.txt|25 &lt;list&gt; 25 ## 23: 2017-01-16 08:00:00|dams_sample.txt|26 &lt;list&gt; 26 ## 24: 2017-01-16 08:00:00|dams_sample.txt|27 &lt;list&gt; 27 ## 25: 2017-01-16 08:00:00|dams_sample.txt|28 &lt;list&gt; 28 ## 26: 2017-01-16 08:00:00|dams_sample.txt|29 &lt;list&gt; 29 ## 27: 2017-01-16 08:00:00|dams_sample.txt|30 &lt;list&gt; 30 ## 28: 2017-01-16 08:00:00|dams_sample.txt|31 &lt;list&gt; 31 ## 29: 2017-01-16 08:00:00|dams_sample.txt|32 &lt;list&gt; 32 ## id file_info region_id ## experiment_id start_datetime ## 1: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 2: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 3: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 4: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 5: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 6: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 7: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 8: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 9: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 10: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 11: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 12: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 13: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 14: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 15: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 16: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 17: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 18: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 19: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 20: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 21: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 22: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 23: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 24: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 25: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 26: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 27: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 28: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## 29: 2017-01-16 08:00:00|dams_sample.txt 2017-01-16 08:00:00 ## experiment_id start_datetime ## stop_datetime period_group uid period power signif_threshold ## 1: 2017-01-26 00:09:00 long 1 99720 276.4555 177.4971 ## 2: 2017-01-26 00:09:00 long 2 100440 315.2950 178.7532 ## 3: 2017-01-26 00:09:00 long 3 100440 257.1494 178.7532 ## 4: 2017-01-26 00:09:00 long 4 95040 268.6532 171.1986 ## 5: 2017-01-26 00:09:00 long 5 100440 195.7411 178.7532 ## 6: 2017-01-26 00:09:00 long 6 96120 360.2246 172.4607 ## 7: 2017-01-26 00:09:00 long 7 98640 200.8368 176.2397 ## 8: 2017-01-26 00:09:00 long 8 99720 280.3768 177.4971 ## 9: 2017-01-26 00:09:00 long 9 101520 309.5947 180.0083 ## 10: 2017-01-26 00:09:00 long 10 103320 246.4732 182.5150 ## 11: 2017-01-26 00:09:00 short 11 68040 285.3857 132.6670 ## 12: 2017-01-26 00:09:00 short 12 69840 177.5452 135.2830 ## 13: 2017-01-26 00:09:00 short 13 67320 307.1445 131.3561 ## 14: 2017-01-26 00:09:00 short 14 69120 193.0421 133.9760 ## 15: 2017-01-26 00:09:00 short 16 69120 151.6117 133.9760 ## 16: 2017-01-26 00:09:00 short 17 69840 225.2747 135.2830 ## 17: 2017-01-26 00:09:00 short 20 69840 187.8997 135.2830 ## 18: 2017-01-26 00:09:00 short 21 68040 170.8709 132.6670 ## 19: 2017-01-26 00:09:00 wt 22 85320 176.0910 157.2301 ## 20: 2017-01-26 00:09:00 wt 23 88920 182.2338 162.3284 ## 21: 2017-01-26 00:09:00 wt 24 84240 204.4373 155.9519 ## 22: 2017-01-26 00:09:00 wt 25 86040 235.2866 158.5068 ## 23: 2017-01-26 00:09:00 wt 26 88920 233.0950 162.3284 ## 24: 2017-01-26 00:09:00 wt 27 87120 272.0614 159.7821 ## 25: 2017-01-26 00:09:00 wt 28 88920 273.3706 162.3284 ## 26: 2017-01-26 00:09:00 wt 29 88920 169.5197 162.3284 ## 27: 2017-01-26 00:09:00 wt 30 88920 301.1270 162.3284 ## 28: 2017-01-26 00:09:00 wt 31 86040 293.7774 158.5068 ## 29: 2017-01-26 00:09:00 wt 32 88920 168.8304 162.3284 ## stop_datetime period_group uid period power signif_threshold ## p_value peak ## 1: 4.100760e-16 1 ## 2: 3.074761e-21 1 ## 3: 1.989846e-13 1 ## 4: 4.264752e-16 1 ## 5: 1.676299e-06 1 ## 6: 3.756396e-29 1 ## 7: 2.809713e-07 1 ## 8: 1.232934e-16 1 ## 9: 3.295280e-20 1 ## 10: 1.374486e-11 1 ## 11: 5.399920e-26 1 ## 12: 9.984131e-10 1 ## 13: 7.385662e-30 1 ## 14: 6.120254e-12 1 ## 15: 8.433567e-07 1 ## 16: 3.187945e-16 1 ## 17: 4.674815e-11 1 ## 18: 2.936584e-09 1 ## 19: 8.538032e-07 1 ## 20: 7.089435e-07 1 ## 21: 3.618745e-10 1 ## 22: 1.070912e-13 1 ## 23: 7.960386e-13 1 ## 24: 1.593002e-18 1 ## 25: 2.955673e-18 1 ## 26: 1.328092e-05 1 ## 27: 2.880738e-22 1 ## 28: 6.431675e-22 1 ## 29: 1.546028e-05 1 ## p_value peak summary_dt can be used as a standard R dataframe for further analysis. For instance, with ggplot, I make a boxplot showing the distribution of periods, in h, for each groups. I also add a point for each animal (so we see outliers). Then, I make the size of the points proportional to the relative power of the peak discovered, so we get an idea of how much to “trust” this point. ggplot(summary_dt, aes(period_group, period, fill= period_group)) + geom_boxplot(outlier.colour = NA) + geom_jitter(aes(size=power - signif_threshold), alpha=.5) + scale_y_hours(name = &quot;Period (h)&quot;) Another direction could be to perform pairwise wilcoxon tests between groups: pairwise.wilcox.test(summary_dt$period, summary_dt$period_group ) ## Warning in wilcox.test.default(xi, xj, paired = paired, ...): cannot ## compute exact p-value with ties ## Warning in wilcox.test.default(xi, xj, paired = paired, ...): cannot ## compute exact p-value with ties ## Warning in wilcox.test.default(xi, xj, paired = paired, ...): cannot ## compute exact p-value with ties ## ## Pairwise comparisons using Wilcoxon rank sum test ## ## data: summary_dt$period and summary_dt$period_group ## ## long short ## short 5e-04 - ## wt 3e-04 5e-04 ## ## P value adjustment method: holm This tells us that there is a statistically significant difference between each pair of groups. Next steps Visualise data with ggetho Sleep analysis sleepr "],
["community.html", "Issues and community Citation Having troubles Contributing", " Issues and community Citation retomics is an academic software. As such, it is – and will always be – completely free and open source. If you use rethomics as part of your research, please cite our reference publication. Having troubles If you are having issues or you want help with something, the best thing you can do is fill an “issue” on the github repository of the relevant package. For instance, if you are struggling to load DAMS data, you should go to the the damr package. This way, you can see if other users have had the same issue, and if we are already working on it! Contributing We welcome external contributions and hope rethomics develops as a community. There are several ways you can contribute: By requesting features and reporting bugs through the github issue system By sending pull requests to preexisting packages By volunteering to develop a new package (for instance to import a new type of data) "]
]
